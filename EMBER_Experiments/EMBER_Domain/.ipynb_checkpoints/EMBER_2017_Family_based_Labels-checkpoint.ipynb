{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for task 2017-01\n",
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.10.0-845f675 found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n",
      "Vectorizing 2017-01 task data\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'avclass'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3fb6410267fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Processing data for task {current_task}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;31m#print(current_task, task_months)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mcreate_task_based_vectorized_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_months\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0mread_task_based_vectorized_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3fb6410267fe>\u001b[0m in \u001b[0;36mcreate_task_based_vectorized_features\u001b[0;34m(data_dir, save_dir, current_task, task_months, feature_version)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mraw_feature_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_feature_paths_base_tr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mraw_feature_paths_base_te\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_num_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_feature_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;31m#print(nrows)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3fb6410267fe>\u001b[0m in \u001b[0;36mtask_num_rows\u001b[0;34m(raw_feature_paths, task_months)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mraw_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mraw_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'appeared'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtask_months\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0mfamily_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'avclass'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                     \u001b[0mcnt_rows\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcnt_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'avclass'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import multiprocessing\n",
    "from ember_features import PEFeatureExtractor\n",
    "\n",
    "def vectorize(irow, raw_features_string, X_path, y_path, extractor, nrows):\n",
    "    \"\"\"\n",
    "    Vectorize a single sample of raw features and write to a large numpy file\n",
    "    \"\"\"\n",
    "    raw_features = json.loads(raw_features_string)\n",
    "    \n",
    "    feature_vector = extractor.process_raw_features(raw_features)\n",
    "\n",
    "    y = np.memmap(y_path, dtype=np.float32, mode=\"r+\", shape=nrows)\n",
    "    y[irow] = raw_features[\"label\"]\n",
    "    \n",
    "\n",
    "    X = np.memmap(X_path, dtype=np.float32, mode=\"r+\", shape=(nrows, extractor.dim))\n",
    "    X[irow] = feature_vector\n",
    "\n",
    "\n",
    "def vectorize_unpack(args):\n",
    "    \"\"\"\n",
    "    Pass through function for unpacking vectorize arguments\n",
    "    \"\"\"\n",
    "    return vectorize(*args)\n",
    "\n",
    "\n",
    "\n",
    "def create_parent_folder(file_path):\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        os.makedirs(os.path.dirname(file_path))\n",
    "        \n",
    "def raw_feature_iterator(file_paths, task_months):\n",
    "    \"\"\"\n",
    "    Yield raw feature strings from the inputed file paths\n",
    "    \"\"\"\n",
    "    for path in file_paths:\n",
    "        with open(path, \"r\") as fin:\n",
    "            for line in fin:\n",
    "                raw_features = json.loads(line)\n",
    "                if raw_features['appeared'] in task_months:\n",
    "                    yield line\n",
    "\n",
    "\n",
    "def task_based_vectorize_subset(X_path, y_path, raw_feature_paths, task_months, extractor, nrows):\n",
    "    \"\"\"\n",
    "    Vectorize a subset of data and write it to disk\n",
    "    \"\"\"\n",
    "    # Create space on disk to write features to\n",
    "    X = np.memmap(X_path, dtype=np.float32, mode=\"w+\", shape=(nrows, extractor.dim))\n",
    "    y = np.memmap(y_path, dtype=np.float32, mode=\"w+\", shape=nrows)\n",
    "\n",
    "    del X, y\n",
    "\n",
    "    # Distribute the vectorization work\n",
    "    pool = multiprocessing.Pool()\n",
    "    argument_iterator = ((irow, raw_features_string, X_path, y_path, extractor, nrows)\n",
    "                         for irow, raw_features_string in enumerate(raw_feature_iterator(raw_feature_paths, task_months)))\n",
    "    #print(argument_iterator)\n",
    "    \n",
    "    \n",
    "    for _ in tqdm.tqdm(pool.imap_unordered(vectorize_unpack, argument_iterator), total=nrows):\n",
    "        pass\n",
    "    \n",
    "    #return argument_iterator\n",
    "\n",
    "        \n",
    "def task_num_rows(raw_feature_paths, task_months):\n",
    "    cnt_rows = 0\n",
    "    \n",
    "    family_labels = []\n",
    "    \n",
    "    for fp in raw_feature_paths:\n",
    "        #print(fp)\n",
    "        with open(fp, \"r\") as fin:\n",
    "            print(fp)\n",
    "            for line in fin:\n",
    "                raw_features = json.loads(line)\n",
    "                if raw_features['appeared'] in task_months:\n",
    "                    family_labels.append(raw_features['avclass'])\n",
    "                    cnt_rows += 1\n",
    "    return cnt_rows, family_labels\n",
    "\n",
    "\n",
    "def create_task_based_vectorized_features(data_dir, save_dir, current_task, task_months, feature_version=2):\n",
    "    \"\"\"\n",
    "    Create feature vectors from raw features and write them to disk\n",
    "    \"\"\"\n",
    "    extractor = PEFeatureExtractor(feature_version)\n",
    "    \n",
    "    print(f'Vectorizing {current_task} task data')\n",
    "    X_path = os.path.join(save_dir, \"X_train.dat\")\n",
    "    y_path = os.path.join(save_dir, \"y_train.dat\")\n",
    "    \n",
    "    #y_path_family_labels = os.path.join(save_dir, \"y_family_train.dat\")\n",
    "    \n",
    "    raw_feature_paths_base_tr = [os.path.join(data_dir, \"train_features_{}.jsonl\".format(i)) for i in range(6)]\n",
    "    raw_feature_paths_base_te = [os.path.join(data_dir, \"test_features.jsonl\")]\n",
    "    raw_feature_paths = raw_feature_paths_base_tr + raw_feature_paths_base_te\n",
    "    \n",
    "    nrows, family_labels = task_num_rows(raw_feature_paths, current_task)\n",
    "    #print(nrows)\n",
    "    \n",
    "    save_test_file = save_dir + 'task_family_labels.npz'\n",
    "    np.savez(save_test_file, family_labels=family_labels)\n",
    "    \n",
    "    \n",
    "    task_based_vectorize_subset(X_path, y_path, raw_feature_paths, current_task, extractor, nrows)\n",
    "    #argument_iterator = task_based_vectorize_subset(X_path, y_path, raw_feature_paths, task_months, extractor, nrows)\n",
    "    \n",
    "    #return argument_iterator\n",
    "\n",
    "def read_task_based_vectorized_features(save_dir, feature_version=2):\n",
    "    \"\"\"\n",
    "    Read vectorized features into memory mapped numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    extractor = PEFeatureExtractor(feature_version)\n",
    "    ndim = extractor.dim\n",
    "    X_ = None\n",
    "    y_ = None\n",
    "\n",
    "\n",
    "    X_path = os.path.join(save_dir, \"X_train.dat\")\n",
    "    y_path = os.path.join(save_dir, \"y_train.dat\")\n",
    "    \n",
    "    y_ = np.memmap(y_path, dtype=np.float32, mode=\"r\")\n",
    "    N = y_.shape[0]\n",
    "    \n",
    "    X_ = np.memmap(X_path, dtype=np.float32, mode=\"r\", shape=(N, ndim))\n",
    "    \n",
    "    print(np.unique(y_))\n",
    "    \n",
    "    goodware_indices = []\n",
    "    malware_indices = []\n",
    "    \n",
    "    \n",
    "    for ind, i in enumerate(y_):\n",
    "        if i == 0:\n",
    "            goodware_indices.append(ind)\n",
    "        elif i == 1:\n",
    "            malware_indices.append(ind)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    malware_goodware_indices = goodware_indices + malware_indices\n",
    "    \n",
    "    print(len(y_[malware_goodware_indices]), len(y_[goodware_indices]), len(y_[malware_indices]))\n",
    "    \n",
    "    \n",
    "    Y_family_labels_file = save_dir + 'task_family_labels.npz'\n",
    "    Y_fam_labels_ = np.load(Y_family_labels_file)\n",
    "    Y_fam_labels = Y_fam_labels_['family_labels']\n",
    "    \n",
    "    \n",
    "    X = X_[malware_goodware_indices]\n",
    "    Y = y_[malware_goodware_indices]\n",
    "    Y_families = Y_fam_labels[malware_goodware_indices]\n",
    "    \n",
    "    indx = [i for i in range(len(Y))]\n",
    "    random.shuffle(indx)\n",
    "\n",
    "    train_size = int(len(indx)*0.9)\n",
    "    trainset = indx[:train_size]\n",
    "    testset = indx[train_size:]\n",
    "\n",
    "    # Separate the training set\n",
    "    X_train = X[trainset]\n",
    "    Y_train = Y[trainset]\n",
    "    Y_family_train = Y_families[trainset]\n",
    "\n",
    "    # Separate the test set\n",
    "    X_test = X[testset]\n",
    "    Y_test = Y[testset]\n",
    "    Y_family_test = Y_families[testset]\n",
    "    \n",
    "    \n",
    "    print(f'X_train {X_train.shape} Y_train {Y_train.shape} Y_family_train {Y_family_train.shape}\\n X_test {X_test.shape} Y_test {Y_test.shape} \\n Y_family_test {Y_family_test.shape}')\n",
    "    \n",
    "    print(f'saving files ...')\n",
    "    save_training_file = save_dir + 'XY_train.npz'\n",
    "    save_test_file = save_dir + 'XY_test.npz'\n",
    "    \n",
    "    np.savez(save_training_file, X_train=X_train, Y_train=Y_train, Y_family_train = Y_family_train)\n",
    "    np.savez(save_test_file, X_test=X_test, Y_test=Y_test, Y_family_test = Y_family_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "all_task_months = ['2017-01', '2017-02', '2017-03', '2017-04', '2017-05', '2017-06',\n",
    "                   '2017-07', '2017-08', '2017-09', '2017-10', '2017-11', '2017-12']\n",
    "\n",
    "data_dir = \"../../../ember2017/ember_2017_2/\"\n",
    "\n",
    "\n",
    "\n",
    "for task in range(0,len(all_task_months)):\n",
    "    start_time = time.time()\n",
    "    #task = 5 + task\n",
    "    current_task = all_task_months[task]\n",
    "    task_months = all_task_months[:task+1]\n",
    "    \n",
    "    \n",
    "    save_dir = '../../../ember2017/EMBER2017withFamilyLabels/' + str(current_task) + '/'\n",
    "    create_parent_folder(save_dir)\n",
    "    \n",
    "    print(f'Processing data for task {current_task}')\n",
    "    #print(current_task, task_months)\n",
    "    create_task_based_vectorized_features(data_dir, save_dir, current_task, task_months, feature_version=2)\n",
    "    read_task_based_vectorized_features(save_dir, feature_version=2)\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f'Elapsed time {(end_time - start_time)/60} mins.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
