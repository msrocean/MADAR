{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import multiprocessing\n",
    "from ember_features import PEFeatureExtractor\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as stats\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import hdbscan\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.rcParams['font.size'] = 18\n",
    "#plt.rcParams['font.family'] = \"serif\"\n",
    "tdir = 'in'\n",
    "major = 5.0\n",
    "minor = 3.0\n",
    "plt.rcParams['xtick.direction'] = tdir\n",
    "plt.rcParams['ytick.direction'] = tdir\n",
    "plt.rcParams['xtick.major.size'] = major\n",
    "plt.rcParams['xtick.minor.size'] = minor\n",
    "plt.rcParams['ytick.major.size'] = major\n",
    "plt.rcParams['ytick.minor.size'] = minor\n",
    "\n",
    "np.random.RandomState(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_task_months = ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06',\n",
    "                   '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12']\n",
    "\n",
    "malware_family_sample_count = {}\n",
    "goodware_family_sample_count = {}\n",
    "\n",
    "malware_family_samples = {}\n",
    "\n",
    "others_family = {}\n",
    "\n",
    "standardization = StandardScaler()\n",
    "\n",
    "#task_based_malware_samples = {}\n",
    "for task in range(0,len(all_task_months)):\n",
    "    \n",
    "    current_task = all_task_months[task]\n",
    "    task_months = all_task_months[:task+1]\n",
    "    \n",
    "    \n",
    "    data_dir = '/home/mr6564/continual_research/month_based_processing_with_family_labels/' + str(current_task) + '/'\n",
    "    \n",
    "    #print(f'Processing data for task {current_task}')\n",
    "    Y_family_labels_file = data_dir + 'task_family_labels.npz'\n",
    "    Y_fam_labels_ = np.load(Y_family_labels_file)\n",
    "    Y_fam_labels = Y_fam_labels_['family_labels']\n",
    "\n",
    "    y_path = os.path.join(data_dir, \"y_train.dat\")\n",
    "    X_path = os.path.join(data_dir, \"X_train.dat\")\n",
    "    \n",
    "    \n",
    "    y_ = np.memmap(y_path, dtype=np.float32, mode=\"r\")\n",
    "    N = y_.shape[0]\n",
    "    \n",
    "    ndim = 2381\n",
    "    X_ = np.memmap(X_path, dtype=np.float32, mode=\"r\", shape=(N, ndim))    \n",
    "    #print(np.unique(y_))\n",
    "    \n",
    "    Y_family_labels_file = data_dir + 'task_family_labels.npz'\n",
    "    Y_fam_labels_ = np.load(Y_family_labels_file)\n",
    "    Y_fam_labels = Y_fam_labels_['family_labels']\n",
    "    \n",
    "    ##standardize \n",
    "    standard_scaler = standardization.partial_fit(X_)\n",
    "    X_ = standard_scaler.transform(X_)\n",
    "    X_ = np.array(X_, np.float32)\n",
    "    print(f'task {current_task} samples {len(X_)}')\n",
    "    \n",
    "    goodware_indices = []\n",
    "    malware_indices = []\n",
    "    \n",
    "    others_family_samples = []\n",
    "    \n",
    "    malware_task_family_samples = defaultdict(list)\n",
    "    \n",
    "    for ind, i in enumerate(y_):\n",
    "        if i == 0:\n",
    "            goodware_indices.append(ind)\n",
    "        elif i == 1:\n",
    "            malware_indices.append(ind)\n",
    "            if Y_fam_labels[ind] == '':\n",
    "                others_family_samples.append(X_[ind])\n",
    "            else:\n",
    "                malware_task_family_samples[Y_fam_labels[ind]].append(X_[ind])\n",
    "                \n",
    "                \n",
    "                #if Y_fam_labels[ind] not in malware_task_family_samples.keys():\n",
    "                #    malware_task_family_samples[Y_fam_labels[ind]]= X_ind\n",
    "                #else:\n",
    "                #    malware_task_family_samples[Y_fam_labels[ind]].append(X_ind)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    malware_family_samples[task] = malware_task_family_samples\n",
    "    others_family[task] = others_family_samples\n",
    "    \n",
    "\n",
    "    Y_families_malware = Y_fam_labels[malware_indices]\n",
    "    Y_families_goodware = Y_fam_labels[goodware_indices]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for yfam in Y_families_malware:\n",
    "        if yfam in malware_family_sample_count.keys():\n",
    "            malware_family_sample_count[yfam] += 1\n",
    "        else:\n",
    "            malware_family_sample_count[yfam] = 1\n",
    "    \n",
    "    \n",
    "    for yfam in Y_families_goodware:\n",
    "        if yfam in goodware_family_sample_count.keys():\n",
    "            goodware_family_sample_count[yfam] += 1\n",
    "        else:\n",
    "            goodware_family_sample_count[yfam] = 1\n",
    "            \n",
    "            \n",
    "\n",
    "print(len(malware_family_sample_count.keys()), len(goodware_family_sample_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = ['xtrat', 'zbot', 'ramnit', 'sality', 'installmonster',\\\n",
    "              'zusy', 'emotet', 'vtflooder', 'others_family', 'fareit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_HDBSCAN_train_test(task_id, family_ind, task_families_data):\n",
    "\n",
    "    top_10 = ['xtrat', 'zbot', 'ramnit', 'sality', 'installmonster',\\\n",
    "                  'zusy', 'emotet', 'vtflooder', 'others_family', 'fareit']\n",
    "    all_task_months = ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06',\n",
    "                   '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12']\n",
    "\n",
    "    curr_task = all_task_months[task_id]\n",
    "    family_name = top_10[family_ind]\n",
    "\n",
    "    print(f'task {curr_task} family name {family_name}')\n",
    "    data_X = task_families_data[task_id][family_name]\n",
    "\n",
    "    clf = hdbscan.HDBSCAN()\n",
    "    clf_labels = clf.fit(data_X)\n",
    "\n",
    "    labels = clf_labels.labels_\n",
    "    unique_labels = np.unique(labels)\n",
    "    num_unique_labels = len(unique_labels)  \n",
    "\n",
    "\n",
    "\n",
    "    valid_clusters_samples = []\n",
    "    valid_clusters_labels = []\n",
    "    cnt = 0\n",
    "    for ulabel in unique_labels:\n",
    "        if ulabel != -1:\n",
    "            ulabel_indx = np.where(labels == ulabel)\n",
    "            #print(ulabel_indx)\n",
    "            #print(f'ulabel {ulabel} samples-{len(ulabel_indx)}')\n",
    "            if len(ulabel_indx[0]) >= 10:\n",
    "                data_X = np.array(data_X)\n",
    "                #print(len(ulabel_indx[0]))\n",
    "                ulabel_samples = data_X[ulabel_indx]\n",
    "\n",
    "                for ulabelsample in ulabel_samples:\n",
    "                    valid_clusters_samples.append(ulabelsample)\n",
    "                    valid_clusters_labels.append(cnt)\n",
    "\n",
    "                cnt += 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    valid_clusters_samples = np.array(valid_clusters_samples)\n",
    "    valid_clusters_labels = np.array(valid_clusters_labels)\n",
    "\n",
    "    print(f'valid clusters {cnt} among {len(unique_labels)}')\n",
    "    print(f'{len(valid_clusters_labels) == len(valid_clusters_samples)}')\n",
    "\n",
    "\n",
    "\n",
    "    samples_indx = list(range(len(valid_clusters_labels)))\n",
    "    random.shuffle(samples_indx)\n",
    "\n",
    "    train_size = int(len(samples_indx)*0.9)\n",
    "    trainset = samples_indx[:train_size]\n",
    "    testset = samples_indx[train_size:]\n",
    "\n",
    "    # Separate the training set\n",
    "    X_tr = valid_clusters_samples[np.array(trainset)]\n",
    "    Y_tr = valid_clusters_labels[np.array(trainset)]\n",
    "\n",
    "    # Separate the test set\n",
    "    X_te = valid_clusters_samples[np.array(testset)]\n",
    "    Y_te = valid_clusters_labels[np.array(testset)]\n",
    "\n",
    "\n",
    "    return X_tr, Y_tr, X_te, Y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_tr, Y_tr, X_te, Y_te = get_HDBSCAN_train_test(0, 6, task_families_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from ember_utils import *\n",
    "from ember_model import *\n",
    "from ember_pjr_utils import *\n",
    "\n",
    "\n",
    "\n",
    "patience = 10\n",
    "num_exps = 1\n",
    "num_epoch = 500\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "exp_type = replay_type = 'testHDBSCAN-Quality'\n",
    "exp_seeds = [random.randint(1, 99999) for i in range(1)]\n",
    "exp = exp_seeds[0]\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "use_cuda = True\n",
    "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
    "use_cuda = use_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "torch.manual_seed(exp)\n",
    "\n",
    "model = Ember_MLP_Net()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.000001)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "print(f'Model has {count_parameters(model)/1000000}m parameters')    \n",
    "criterion = nn.BCELoss()    \n",
    "\n",
    "family_ind = 6\n",
    "\n",
    "for task_month in range(len(all_task_months)):\n",
    "\n",
    "    \n",
    "    task_start = time.time()\n",
    "\n",
    "    current_task = all_task_months[task_month]\n",
    "    task_months = all_task_months[:task_month+1]\n",
    "    \n",
    "    model_save_dir = '../HDBSCAN_Quality' +\\\n",
    "                    str(exp_type) + '/HDBSCAN_Quality' + str(current_task) + '/'\n",
    "    create_parent_folder(model_save_dir)\n",
    "\n",
    "    opt_save_path = '../HDBSCAN_Quality' +\\\n",
    "                str(exp_type) + '/HDBSCAN_Quality' + str(current_task) + '/'\n",
    "    create_parent_folder(opt_save_path)\n",
    "\n",
    "\n",
    "    results_save_dir = './HDBSCAN_Quality' +\\\n",
    "                str(exp_type) + '/HDBSCAN_Quality' + '/' \n",
    "    create_parent_folder(results_save_dir)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train, Y_train, X_test, Y_test = get_HDBSCAN_train_test(task_month, family_ind, malware_family_samples)\n",
    "\n",
    "    print()\n",
    "    print(f'X_train {X_train.shape} Y_train {Y_train.shape}')\n",
    "    print()\n",
    "\n",
    "    task_training_time, epoch_ran, training_loss, validation_loss  =\\\n",
    "                            training_early_stopping(model, model_save_dir, opt_save_path,\\\n",
    "                            X_train, Y_train, X_test, Y_test, patience,\\\n",
    "                            batch_size, device, optimizer, num_epoch,\\\n",
    "                             criterion, replay_type, current_task, exp, earlystopping=True)\n",
    "\n",
    "\n",
    "\n",
    "    model = Ember_MLP_Net()\n",
    "    model = model.to(device)\n",
    "    #load the best model for this task\n",
    "    best_model_path = model_save_dir + os.listdir(model_save_dir)[0]\n",
    "    print(f'loading best model {best_model_path}')\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.000001)\n",
    "    best_optimizer = opt_save_path + os.listdir(opt_save_path)[0]\n",
    "    print(f'loading best optimizer {best_optimizer}')\n",
    "    optimizer.load_state_dict(torch.load(best_optimizer))\n",
    "\n",
    "\n",
    "    acc, rocauc = testing_aucscore(model, X_test, Y_test, batch_size, device)\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f'Elapsed time {(end_time - start_time)/60} mins.')    \n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "cnt += 1\n",
    "print(f'Elapsed time {(end_time - start_time)/60} mins.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
