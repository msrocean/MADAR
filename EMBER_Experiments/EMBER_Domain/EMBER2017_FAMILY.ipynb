{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import multiprocessing\n",
    "from ember_features import PEFeatureExtractor\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import operator\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emberdata_family_stat(indicate_month):\n",
    "    data_dir = \"../../../ember2017/ember_2017_2/\"\n",
    "    raw_feature_paths_base_tr = [os.path.join(data_dir, \"train_features_{}.jsonl\".format(i)) for i in range(6)]\n",
    "    raw_feature_paths_base_te = [os.path.join(data_dir, \"test_features.jsonl\")]\n",
    "    raw_feature_paths = raw_feature_paths_base_tr + raw_feature_paths_base_te\n",
    "    #print(raw_feature_paths)\n",
    "\n",
    "    all_task_months = ['2017-01', '2017-02', '2017-03', '2017-04', '2017-05', '2017-06',\n",
    "                   '2017-07', '2017-08', '2017-09', '2017-10', '2017-11', '2017-12']\n",
    "    task_months = all_task_months[indicate_month]\n",
    "\n",
    "    av_class_stats = {}\n",
    "    cnt_rows = 0\n",
    "    cnt_good_rows = 0\n",
    "    cnt_missing_rows = 0\n",
    "\n",
    "    for fp in raw_feature_paths:\n",
    "        #print(fp)\n",
    "        with open(fp, \"r\") as fin:\n",
    "            #print(fp)\n",
    "            for line in fin:\n",
    "                raw_features = json.loads(line)\n",
    "                #print(raw_features.keys())\n",
    "\n",
    "                if raw_features['appeared'] in task_months:\n",
    "                    if raw_features['label'] == 1: # and raw_features['avclass']\n",
    "                        #print(raw_features['label'], raw_features['avclass'])\n",
    "                        try:\n",
    "                            if raw_features['avclass'] not in av_class_stats.keys():\n",
    "                                av_class_stats[raw_features['avclass']] = 1\n",
    "                            else:\n",
    "                                av_class_stats[raw_features['avclass']] += 1\n",
    "                        except:\n",
    "                            if 'missing_avclass' not in av_class_stats.keys():\n",
    "                                av_class_stats['missing_avclass'] = 1\n",
    "                            else:\n",
    "                                av_class_stats['missing_avclass'] += 1\n",
    "                                \n",
    "                                \n",
    "                        cnt_rows += 1\n",
    "\n",
    "                    elif raw_features['label'] == 0:\n",
    "                        cnt_good_rows += 1\n",
    "                    elif raw_features['label'] == -1:\n",
    "                        #print(raw_features['label'], raw_features['avclass'])\n",
    "                        cnt_missing_rows += 1\n",
    "\n",
    "            #if cnt_rows == 2:\n",
    "            #    break\n",
    "    \n",
    "    \n",
    "    \n",
    "    min_samples = 0\n",
    "    \n",
    "    families_more_than_400_samples = {}\n",
    "\n",
    "    for k, v in av_class_stats.items():\n",
    "        if v >= min_samples and k != '':\n",
    "            families_more_than_400_samples[k] = v\n",
    "    \n",
    "    \n",
    "    stat_list = np.array(list(av_class_stats.values()))\n",
    "    #print(f' Samples {sum(stat_list)} families {len(av_class_stats.keys())}')\n",
    "    print(f'Month {task_months} has {cnt_rows} malware samples \\n belonging to {len(av_class_stats.keys())} families, goodware {cnt_good_rows}') \n",
    "    return families_more_than_400_samples, av_class_stats\n",
    "\n",
    "#families_more_than_400_samples, av_class_stats = get_emberdata_family_stat(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-01 has 32761 malware samples \n",
      " belonging to 1 families, goodware 17180\n",
      "\n",
      "********************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    families_more_than_400_samples, av_class_stats = get_emberdata_family_stat(i)\n",
    "    print()\n",
    "    print('********************')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_list = np.array(list(av_class_stats.values()))\n",
    "print(sum(stat_list))\n",
    "\n",
    "min_samples = 0\n",
    "\n",
    "valid_families = {}\n",
    "\n",
    "for k, v in av_class_stats.items():\n",
    "    if v >= min_samples and k != '':\n",
    "        valid_families[k] = v\n",
    "        \n",
    "print(sum(np.array(list(valid_families.values()))), len(np.array(list(valid_families.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 400\n",
    "\n",
    "valid_families = {}\n",
    "\n",
    "for k, v in av_class_stats.items():\n",
    "    if v >= min_samples and k != '':\n",
    "        valid_families[k] = v\n",
    "print(sum(np.array(list(valid_families.values()))), len(np.array(list(valid_families.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2899 - 106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_list = np.array(list(av_class_stats.values()))\n",
    "\n",
    "stat_list = [i for i in stat_list if i > 500]\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.bar(range(len(stat_list)),stat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.size'] = 15\n",
    "#plt.rcParams['font.family'] = \"serif\"\n",
    "tdir = 'in'\n",
    "major = 5.0\n",
    "minor = 3.0\n",
    "plt.rcParams['xtick.direction'] = tdir\n",
    "plt.rcParams['ytick.direction'] = tdir\n",
    "plt.rcParams['xtick.major.size'] = major\n",
    "plt.rcParams['xtick.minor.size'] = minor\n",
    "plt.rcParams['ytick.major.size'] = major\n",
    "plt.rcParams['ytick.minor.size'] = minor\n",
    "\n",
    "\n",
    "\n",
    "min_samples = 400\n",
    "\n",
    "\n",
    "families_more_than_400_samples = {}\n",
    "\n",
    "labels = []\n",
    "num_samples = []\n",
    "\n",
    "\n",
    "\n",
    "for k, v in stat_ember:\n",
    "    if v >= min_samples and k != '':\n",
    "        #print(k, v)\n",
    "        labels.append(k)\n",
    "        num_samples.append(v)\n",
    "        families_more_than_400_samples[k] = v\n",
    "        \n",
    "print(len(labels), len(num_samples))\n",
    "\n",
    "'''\n",
    "save_file_name = 'ember_' + str(min_samples) + '_stats.png'\n",
    "#print(x_axis, y_)\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.bar(range(len(labels)), num_samples, color='blue')\n",
    "plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "plt.ylabel('Number of Samples', fontsize=20)\n",
    "plt.xlabel(f'Family Labels', fontsize=20)\n",
    "#plt.grid(True)\n",
    "plt.savefig('./store_figures/' + save_file_name,\n",
    "        bbox_inches='tight', \n",
    "       transparent=True);\n",
    "       \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# family, bad, good, unknown\n",
    "#(2900, 400000, 350000, 200000)\n",
    "# 11433 missing avclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(irow, raw_features_string, X_path, y_path, extractor, nrows):\n",
    "    \"\"\"\n",
    "    Vectorize a single sample of raw features and write to a large numpy file\n",
    "    \"\"\"\n",
    "    raw_features = json.loads(raw_features_string)\n",
    "    \n",
    "    feature_vector = extractor.process_raw_features(raw_features)\n",
    "\n",
    "    y = np.memmap(y_path, dtype=np.float32, mode=\"r+\", shape=nrows)\n",
    "    y[irow] = top_families_100_labels[raw_features[\"avclass\"]]\n",
    "\n",
    "    X = np.memmap(X_path, dtype=np.float32, mode=\"r+\", shape=(nrows, extractor.dim))\n",
    "    X[irow] = feature_vector\n",
    "\n",
    "\n",
    "def vectorize_unpack(args):\n",
    "    \"\"\"\n",
    "    Pass through function for unpacking vectorize arguments\n",
    "    \"\"\"\n",
    "    return vectorize(*args)\n",
    "\n",
    "\n",
    "\n",
    "def create_parent_folder(file_path):\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        os.makedirs(os.path.dirname(file_path))\n",
    "        \n",
    "def raw_feature_iterator(file_paths, top_families):\n",
    "    \"\"\"\n",
    "    Yield raw feature strings from the inputed file paths\n",
    "    \"\"\"\n",
    "    all_task_months = ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06',\n",
    "                   '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12']\n",
    "    \n",
    "    for path in file_paths:\n",
    "        with open(path, \"r\") as fin:\n",
    "            for line in fin:\n",
    "                raw_features = json.loads(line)\n",
    "                if raw_features['appeared'] in all_task_months:\n",
    "                    if raw_features['avclass'] != '':\n",
    "                        if raw_features['avclass'] in top_families and raw_features['label'] == 1:\n",
    "                            yield line\n",
    "\n",
    "\n",
    "def task_based_vectorize_subset(X_path, y_path, raw_feature_paths, top_families, extractor, nrows):\n",
    "    \"\"\"\n",
    "    Vectorize a subset of data and write it to disk\n",
    "    \"\"\"\n",
    "    # Create space on disk to write features to\n",
    "    X = np.memmap(X_path, dtype=np.float32, mode=\"w+\", shape=(nrows, extractor.dim))\n",
    "    y = np.memmap(y_path, dtype=np.float32, mode=\"w+\", shape=nrows)\n",
    "    del X, y\n",
    "\n",
    "    # Distribute the vectorization work\n",
    "    pool = multiprocessing.Pool()\n",
    "    argument_iterator = ((irow, raw_features_string, X_path, y_path, extractor, nrows)\n",
    "                         for irow, raw_features_string in enumerate(raw_feature_iterator(raw_feature_paths, top_families)))\n",
    "    #print(argument_iterator)\n",
    "    \n",
    "    \n",
    "    for _ in tqdm.tqdm(pool.imap_unordered(vectorize_unpack, argument_iterator), total=nrows):\n",
    "        pass\n",
    "    \n",
    "    #return argument_iterator\n",
    "\n",
    "        \n",
    "def task_num_rows(raw_feature_paths, top_families):\n",
    "    print(top_families)\n",
    "    all_task_months = ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06',\n",
    "                   '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12']\n",
    "    cnt_rows = 0\n",
    "    for fp in raw_feature_paths:\n",
    "        #print(fp)\n",
    "        with open(fp, \"r\") as fin:\n",
    "            #print(fp)\n",
    "            for line in fin:\n",
    "                raw_features = json.loads(line)\n",
    "                if raw_features['appeared'] in all_task_months:\n",
    "                    if raw_features['avclass'] != '':\n",
    "                        if raw_features['avclass'] in top_families and raw_features['label'] == 1:\n",
    "                            cnt_rows += 1\n",
    "    return cnt_rows\n",
    "\n",
    "\n",
    "def create_task_based_vectorized_features(data_dir, save_dir, top_families, feature_version=2):\n",
    "    \"\"\"\n",
    "    Create feature vectors from raw features and write them to disk\n",
    "    \"\"\"\n",
    "    extractor = PEFeatureExtractor(feature_version)\n",
    "    \n",
    "    #print(f'Vectorizing {current_task} task data')\n",
    "    X_path = os.path.join(save_dir, \"X_train.dat\")\n",
    "    y_path = os.path.join(save_dir, \"y_train.dat\")\n",
    "    raw_feature_paths_base_tr = [os.path.join(data_dir, \"train_features_{}.jsonl\".format(i)) for i in range(6)]\n",
    "    raw_feature_paths_base_te = [os.path.join(data_dir, \"test_features.jsonl\")]\n",
    "    raw_feature_paths = raw_feature_paths_base_tr + raw_feature_paths_base_te\n",
    "    \n",
    "    \n",
    "    \n",
    "    nrows = task_num_rows(raw_feature_paths, top_families)\n",
    "    #print(nrows)\n",
    "    task_based_vectorize_subset(X_path, y_path, raw_feature_paths, top_families, extractor, nrows)\n",
    "    #argument_iterator = task_based_vectorize_subset(X_path, y_path, raw_feature_paths, task_months, extractor, nrows)\n",
    "    \n",
    "    #return argument_iterator\n",
    "\n",
    "def read_task_based_vectorized_features(save_dir, feature_version=2):\n",
    "    \"\"\"\n",
    "    Read vectorized features into memory mapped numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    extractor = PEFeatureExtractor(feature_version)\n",
    "    ndim = extractor.dim\n",
    "    X_ = None\n",
    "    y_ = None\n",
    "\n",
    "\n",
    "    X_path = os.path.join(save_dir, \"X_train.dat\")\n",
    "    y_path = os.path.join(save_dir, \"y_train.dat\")\n",
    "    \n",
    "    y_ = np.memmap(y_path, dtype=np.float32, mode=\"r\")\n",
    "    N = y_.shape[0]\n",
    "    \n",
    "    X_ = np.memmap(X_path, dtype=np.float32, mode=\"r\", shape=(N, ndim))\n",
    "    \n",
    "    print(np.unique(y_))\n",
    "    \n",
    "    X, Y = X_, y_\n",
    "    \n",
    "    indx = [i for i in range(len(Y))]\n",
    "    random.shuffle(indx)\n",
    "\n",
    "    train_size = int(len(indx)*0.9)\n",
    "    trainset = indx[:train_size]\n",
    "    testset = indx[train_size:]\n",
    "\n",
    "    # Separate the training set\n",
    "    X_train = X[trainset]\n",
    "    Y_train = Y[trainset]\n",
    "\n",
    "    # Separate the test set\n",
    "    X_test = X[testset]\n",
    "    Y_test = Y[testset]\n",
    "    \n",
    "    \n",
    "    print(f'X_train {X_train.shape} Y_train {Y_train.shape} X_test {X_test.shape} Y_test {Y_test.shape}')\n",
    "    \n",
    "    print(f'saving files ...')\n",
    "    save_training_file = save_dir + 'XY_train.npz'\n",
    "    save_test_file = save_dir + 'XY_test.npz'\n",
    "    \n",
    "    np.savez(save_training_file, X_train=X_train, Y_train=Y_train)\n",
    "    np.savez(save_test_file, X_test=X_test, Y_test=Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordered_106_families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordered_100_families_keys_100\n",
    "families_more_than_400_samples = get_emberdata_family_stat()\n",
    "ordered_106_families = sorted(families_more_than_400_samples.items(),key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "\n",
    "ordered_100_families_keys_100 = []\n",
    "num_classes = 100\n",
    "\n",
    "cnt = 0\n",
    "for j in ordered_106_families:\n",
    "    #print(j[0], j[1])\n",
    "    cnt += 1\n",
    "    ordered_100_families_keys_100.append(j[0])\n",
    "    if cnt == num_classes:\n",
    "        break\n",
    "print(len(ordered_100_families_keys_100))\n",
    "\n",
    "    \n",
    "top_families_100_labels = {}\n",
    "\n",
    "for ind, fam in enumerate(ordered_100_families_keys_100):\n",
    "    top_families_100_labels[fam] = int(ind)\n",
    "    \n",
    "    \n",
    "all_task_months = ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06',\n",
    "                   '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12']\n",
    "\n",
    "data_dir = \"../../ember/ember_data/2018_data/ember2018/\"\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#top_families = ordered_100_families_keys_100\n",
    "\n",
    "save_dir = '../../ember2018/top_class_bases/top_classes_' + str(num_classes) + '/'\n",
    "\n",
    "create_parent_folder(save_dir)\n",
    "\n",
    "\n",
    "create_task_based_vectorized_features(data_dir, save_dir, ordered_100_families_keys_100, feature_version=2)\n",
    "read_task_based_vectorized_features(save_dir, feature_version=2)\n",
    "    \n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Elapsed time {(end_time - start_time)/60} mins.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 class X_train (269810, 2381) Y_train (269810,) X_test (29979, 2381) Y_test (29979,)\n",
    "# 100 class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = PEFeatureExtractor(2)\n",
    "\n",
    "#print(f'Vectorizing {current_task} task data')\n",
    "X_path = os.path.join(save_dir, \"X_train.dat\")\n",
    "y_path = os.path.join(save_dir, \"y_train.dat\")\n",
    "raw_feature_paths_base_tr = [os.path.join(data_dir, \"train_features_{}.jsonl\".format(i)) for i in range(6)]\n",
    "raw_feature_paths_base_te = [os.path.join(data_dir, \"test_features.jsonl\")]\n",
    "raw_feature_paths = raw_feature_paths_base_tr + raw_feature_paths_base_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_families = ordered_100_families_keys_100[:3]\n",
    "top_families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_task_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_num_rows(raw_feature_paths, top_families):\n",
    "    print(top_families)\n",
    "    cnt_rows = 0\n",
    "    for fp in raw_feature_paths:\n",
    "        #print(fp)\n",
    "        with open(fp, \"r\") as fin:\n",
    "            #print(fp)\n",
    "            for line in fin:\n",
    "                raw_features = json.loads(line)\n",
    "                if raw_features['appeared'] in all_task_months:\n",
    "                    if raw_features['avclass'] != '':\n",
    "                        if raw_features['avclass'] in top_families and raw_features['label'] == 1:\n",
    "                            cnt_rows += 1\n",
    "    return cnt_rows\n",
    "\n",
    "\n",
    "#nrows = task_num_rows(raw_feature_paths, ordered_100_families_keys_100[0])\n",
    "#print(nrows)\n",
    "\n",
    "for i in range(5):\n",
    "    #print(ordered_100_families_keys_100[i])\n",
    "    nrows = task_num_rows(raw_feature_paths, ordered_100_families_keys_100[i])\n",
    "    print(nrows)\n",
    "#task_based_vectorize_subset(X_path, y_path, raw_feature_paths, top_families, extractor, nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrat samples 35969\n",
    "zbot samples 24075\n",
    "ramnit samples 20595\n",
    "sality samples 18572\n",
    "installmonster samples 16691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_100_families_keys_100[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../ember/ember_data/2018_data/ember2018/\"\n",
    "raw_feature_paths_base_tr = [os.path.join(data_dir, \"train_features_{}.jsonl\".format(i)) for i in range(6)]\n",
    "raw_feature_paths_base_te = [os.path.join(data_dir, \"test_features.jsonl\")]\n",
    "raw_feature_paths = raw_feature_paths_base_tr + raw_feature_paths_base_te\n",
    "#print(raw_feature_paths)\n",
    "\n",
    "all_task_months = ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06',\n",
    "                   '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12']\n",
    "\n",
    "task_months = all_task_months\n",
    "\n",
    "cnt_rows = 0\n",
    "\n",
    "\n",
    "for fp in raw_feature_paths:\n",
    "    #print(fp)\n",
    "    with open(fp, \"r\") as fin:\n",
    "        #print(fp)\n",
    "        for line in fin:\n",
    "            raw_features = json.loads(line)\n",
    "            #print(raw_features.keys())\n",
    "\n",
    "            if raw_features['appeared'] in task_months:\n",
    "                if raw_features['avclass'] != '':\n",
    "                    if raw_features['label'] == 1 and raw_features['avclass'] in ordered_100_families_keys_100[4:5]:\n",
    "                        cnt_rows += 1\n",
    "print(cnt_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
