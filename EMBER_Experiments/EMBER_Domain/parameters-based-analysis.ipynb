{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "plt.rcParams['font.size'] = 18\n",
    "#plt.rcParams['font.family'] = \"serif\"\n",
    "tdir = 'in'\n",
    "major = 5.0\n",
    "minor = 3.0\n",
    "plt.rcParams['xtick.direction'] = tdir\n",
    "plt.rcParams['ytick.direction'] = tdir\n",
    "plt.rcParams['xtick.major.size'] = major\n",
    "plt.rcParams['xtick.minor.size'] = minor\n",
    "plt.rcParams['ytick.major.size'] = major\n",
    "plt.rcParams['ytick.minor.size'] = minor\n",
    "\n",
    "\n",
    "from ember_utils import *\n",
    "from ember_model import *\n",
    "from ember_pjr_utils import *\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader_weights(X, y, batch_size):\n",
    "    \n",
    "    X_ = torch.from_numpy(np.array(X)).type(torch.FloatTensor)\n",
    "    y_ = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "    \n",
    "    data_tensored = torch.utils.data.TensorDataset(X_,y_)    \n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(data_tensored, batch_size = batch_size,\n",
    "                                              num_workers=1, drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "\n",
    "def get_weights(model, layer, X_, Y_, batch_size, device):\n",
    "    \n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    if layer == 'fc2':\n",
    "        model.fc2.register_forward_hook(get_activation(layer))\n",
    "    elif layer == 'fc3':\n",
    "        model.fc3.register_forward_hook(get_activation(layer))\n",
    "    elif layer == 'fc4':\n",
    "        model.fc4.register_forward_hook(get_activation(layer))\n",
    "    elif layer == 'fc2_bn':\n",
    "        model.fc2_bn.register_forward_hook(get_activation(layer))\n",
    "    elif layer == 'fc3_bn':\n",
    "        model.fc3_bn.register_forward_hook(get_activation(layer))\n",
    "    elif layer == 'fc4_bn':\n",
    "        model.fc4_bn.register_forward_hook(get_activation(layer))\n",
    "    elif layer == 'act2':\n",
    "        model.act2.register_forward_hook(get_activation(layer))\n",
    "    elif layer == 'act3':\n",
    "        model.act3.register_forward_hook(get_activation(layer))\n",
    "    elif layer == 'act4':\n",
    "        model.act4.register_forward_hook(get_activation(layer)) \n",
    "    \n",
    "    dataloader = get_dataloader_weights(X_, Y_, batch_size)   \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in tqdm(dataloader):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_test_pred = model(x_batch)\n",
    "            feats_batch = activation[layer].cpu().numpy()\n",
    "            \n",
    "            for f in feats_batch:\n",
    "                features.append(f)\n",
    " \n",
    "            \n",
    "    assert len(features) == len(X_)      \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "\n",
    "def get_anomalyScoresSamples(fam, weight_data,\\\n",
    "                             raw_data, samples_in_family,\\\n",
    "                             chooseSample = True):\n",
    "    clf = IsolationForest(max_samples=len(weight_data))\n",
    "    clf.fit(weight_data)\n",
    "    #scores_prediction = clf.decision_function(weight_data)\n",
    "    y_pred = clf.predict(weight_data)\n",
    "\n",
    "\n",
    "    anomalous_idx = np.where(y_pred == -1.0)\n",
    "    similar_idx = np.where(y_pred == 1.0)\n",
    "\n",
    "    #print(f'family {fam} anomaly-similar: {len(anomalous_idx[0])} - {len(similar_idx[0])}')\n",
    "    \n",
    "    if chooseSample:\n",
    "        raw_data = np.array(raw_data)\n",
    "        anomaly_samples = raw_data[anomalous_idx]\n",
    "        \n",
    "        remaining_samples_to_pick = int(samples_in_family - len(anomaly_samples))\n",
    "        \n",
    "        if remaining_samples_to_pick == 0 or int(abs(remaining_samples_to_pick)/remaining_samples_to_pick) == -1:\n",
    "            remaining_samples_to_pick = min(int(len(anomaly_samples) * 0.50), abs(remaining_samples_to_pick))\n",
    "        \n",
    "        if remaining_samples_to_pick >= len(similar_idx):\n",
    "            similar_samples = raw_data[similar_idx]\n",
    "        else:\n",
    "            similar_samples_pool = list(raw_data[similar_idx])\n",
    "            similar_samples = random.sample(similar_samples_pool, remaining_samples_to_pick)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            mal_replay_samples = np.concatenate((np.array(anomaly_samples), np.array(similar_samples)))\n",
    "        except:\n",
    "            print(np.array(anomaly_samples).shape, np.array(similar_samples).shape)\n",
    "\n",
    "        return mal_replay_samples\n",
    "    else:\n",
    "        return scores_prediction\n",
    "\n",
    "\n",
    "def get_anomalySamples(family_dict,\\\n",
    "                       samples_in_family,\\\n",
    "                       model, layer, device):\n",
    "    \n",
    "    \n",
    "    \n",
    "    pre_malware_samples = []\n",
    "    for k, v in family_dict.items():\n",
    "        #print(f'family {k} ....')\n",
    "        if k != 'goodware':\n",
    "            if len(v) > samples_in_family:\n",
    "                #print(f'family {k} to IF.')\n",
    "                \n",
    "                k_samples = v\n",
    "                k_Y = np.ones(len(k_samples))\n",
    "                k_weights = get_weights(model, layer, k_samples, k_Y, batch_size, device)\n",
    "                \n",
    "                #print(f'k_samples {len(k_samples)}  k_weights {len(k_weights)}')\n",
    "                \n",
    "                k_selected_samples = get_anomalyScoresSamples(k, k_weights,\\\n",
    "                                                     k_samples, samples_in_family,\\\n",
    "                                                     chooseSample = True)\n",
    "                for sample in k_selected_samples:\n",
    "                    pre_malware_samples.append(sample)\n",
    "                \n",
    "                family_dict[k] = list(k_selected_samples)\n",
    "            else:\n",
    "                for sample in v:\n",
    "                    pre_malware_samples.append(sample)\n",
    "    \n",
    "\n",
    "    if len(family_dict['goodware']) < len(pre_malware_samples):\n",
    "        pre_goodware_samples = random.sample(family_dict['goodware'], len(family_dict['goodware']))\n",
    "    else:\n",
    "        pre_goodware_samples = random.sample(family_dict['goodware'], len(pre_malware_samples))\n",
    "    \n",
    "    family_dict['goodware'] = list(pre_goodware_samples)\n",
    "    \n",
    "    replay_samples = np.concatenate((list(pre_goodware_samples),\\\n",
    "                                     list(pre_malware_samples)))\n",
    "    labels_to_replay = np.concatenate((list(np.zeros(len(pre_goodware_samples))),\\\n",
    "                                       (np.ones(len(pre_malware_samples)))))\n",
    "    \n",
    "    #random.shuffle(replay_samples, labels_to_replay)\n",
    "    from sklearn.utils import shuffle\n",
    "    X_, Y_ = shuffle(replay_samples, labels_to_replay)\n",
    "    \n",
    "    return X_, Y_, family_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_task_months = ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06',\n",
    "                   '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12']\n",
    "\n",
    "data_dir = '../../month_based_processing_with_family_labels/'\n",
    "\n",
    "\n",
    "\n",
    "patience = 5\n",
    "replay_type = 'pjr'\n",
    "\n",
    "\n",
    "\n",
    "num_exps = 1 #args.num_exps\n",
    "#task_month = args.task_month\n",
    "num_epoch = 500 #args.num_epoch\n",
    "batch_size = 6000 #args.batch_size\n",
    "num_samples_per_malware_family = 500\n",
    "\n",
    "layer = 'act4'\n",
    "\n",
    "exp_type = 'weights'\n",
    "\n",
    "exp_seeds = [random.randint(1, 99999) for i in range(num_exps)]\n",
    "\n",
    "\n",
    "expSaveDir = '../WeightsFinal_'\n",
    "resSaveDir = './Weights_'\n",
    "expSaveFile = '/Weights_replay_'\n",
    "\n",
    "\n",
    "raw_anomalyScores_Dict = {}\n",
    "weight_anomalyScores_Dict = {}\n",
    "\n",
    "\n",
    "cnt =  1    \n",
    "for exp in exp_seeds:\n",
    "    start_time = time.time()\n",
    "    use_cuda = True\n",
    "    print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
    "    use_cuda = use_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    torch.manual_seed(exp)\n",
    "\n",
    "    model = Ember_MLP_Net()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.000001)\n",
    "       \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    print(f'Model has {count_parameters(model)/1000000}m parameters')    \n",
    "    criterion = nn.BCELoss()    \n",
    "\n",
    "    \n",
    "    \n",
    "    stored_global_family_dict = defaultdict(list)\n",
    "    \n",
    "    standardization = StandardScaler()\n",
    "    standard_scaler = None\n",
    "    for task_month in range(len(all_task_months)):\n",
    "                \n",
    "        print(f'\\n{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Round {cnt} ...')\n",
    "        task_start = time.time()\n",
    "        \n",
    "        #task_month = task_month\n",
    "        current_task = all_task_months[task_month]\n",
    "        task_months = all_task_months[:task_month+1]\n",
    "        print(f'Current Task {current_task} w/ {num_samples_per_malware_family} samples to Replay per Malware family.')\n",
    "\n",
    "\n",
    "        model_save_dir = str(expSaveDir) + 'model_' +\\\n",
    "                    str(exp_type) + str(expSaveFile) +\\\n",
    "                    str(num_samples_per_malware_family) + '/' + str(current_task) + '/'\n",
    "        create_parent_folder(model_save_dir)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        opt_save_path = str(expSaveDir) + 'optimizer_' +\\\n",
    "                    str(exp_type) + str(expSaveFile) +\\\n",
    "                    str(num_samples_per_malware_family) + '/' + str(current_task) + '/'\n",
    "        create_parent_folder(opt_save_path)\n",
    "        \n",
    "        \n",
    "        results_save_dir = str(resSaveDir) + 'results_' +\\\n",
    "                    str(exp_type) + str(expSaveFile) +\\\n",
    "                    str(num_samples_per_malware_family) + '/' \n",
    "        create_parent_folder(results_save_dir)\n",
    "\n",
    "        dict_save_file = model_save_dir + 'global_family_dict_' + str(task_month) + '.npz'\n",
    "        np.savez_compressed(dict_save_file, dictfile = dict_save_file)\n",
    "        \n",
    "        \n",
    "        #dictFile = np.load(dict_save_file)\n",
    "        #stored_global_family_dict = dictFile['dictfile']\n",
    "        \n",
    "        \n",
    "        X_train, Y_train, Y_train_family = get_family_labeled_month_data(data_dir, current_task)\n",
    "        X_test, Y_test, Y_test_family = get_family_labeled_task_test_data(data_dir, task_months, mlp_net=True)\n",
    "        \n",
    "        stored_global_family_dict = make_family_based_dict(\\\n",
    "                                   X_train, Y_train, Y_train_family,\\\n",
    "                                   current_task, stored_global_family_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if current_task != all_task_months[0]:\n",
    "            X_train, Y_train, stored_global_family_dict = get_anomalySamples(\n",
    "                                                         stored_global_family_dict,\\\n",
    "                                                         num_samples_per_malware_family,\\\n",
    "                                                         model, layer, device)\n",
    "        \n",
    "        \n",
    "        print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Standardizing ...')\n",
    "        standard_scaler = standardization.partial_fit(X_train)\n",
    "\n",
    "        X_train = standard_scaler.transform(X_train)\n",
    "        X_test = standard_scaler.transform(X_test)\n",
    "\n",
    "        X_train, Y_train = np.array(X_train, np.float32), np.array(Y_train, np.int32)\n",
    "        X_test, Y_test = np.array(X_test, np.float32), np.array(Y_test, np.int32)        \n",
    "        \n",
    "        \n",
    "\n",
    "        # to debug\n",
    "        #X_train, Y_train, Y_train_family = X_train[:500], Y_train [:500], Y_train_family[:500]\n",
    "        #X_test, Y_test, Y_test_family = X_test[:50], Y_test[:50], Y_test_family[:50]\n",
    "        \n",
    "        print()\n",
    "        print(f'X_train {X_train.shape} Y_train {Y_train.shape}')\n",
    "        print()\n",
    "        \n",
    "        print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Training ...')\n",
    "        task_training_time, epoch_ran, training_loss, validation_loss  =\\\n",
    "                                training_early_stopping(model, model_save_dir, opt_save_path,\\\n",
    "                                X_train, Y_train, X_test, Y_test, patience,\\\n",
    "                                batch_size, device, optimizer, num_epoch,\\\n",
    "                                 criterion, replay_type, current_task, exp, earlystopping=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        model = Ember_MLP_Net()\n",
    "        model = model.to(device)\n",
    "        #load the best model for this task\n",
    "        best_model_path = model_save_dir + os.listdir(model_save_dir)[0]\n",
    "        print(f'loading best model {best_model_path}')\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        \n",
    "        \n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.000001)\n",
    "        best_optimizer = opt_save_path + os.listdir(opt_save_path)[0]\n",
    "        print(f'loading best optimizer {best_optimizer}')\n",
    "        optimizer.load_state_dict(torch.load(best_optimizer))\n",
    "        \n",
    "        acc, rocauc = testing_aucscore(model, X_test, Y_test, batch_size, device)\n",
    "        \n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f'Elapsed time {(end_time - start_time)/60} mins.')    \n",
    "        \n",
    "\n",
    "        task_end = time.time()\n",
    "        task_run_time = (task_end - task_start)/60\n",
    "        \n",
    "        \n",
    "        num_replay_samples = len(X_train)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        \n",
    "        results_f = open(os.path.join(results_save_dir + layer + '_weight_final_' + layer + '_' + str(num_samples_per_malware_family) + '_results.txt'), 'a')\n",
    "        result_string = '{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t\\n'.format(current_task,epoch_ran, task_training_time, acc, rocauc, num_replay_samples)\n",
    "        results_f.write(result_string)\n",
    "        results_f.flush()\n",
    "        results_f.close()\n",
    "        \n",
    "    \n",
    "    end_time = time.time()\n",
    "    cnt += 1\n",
    "    print(f'Elapsed time {(end_time - start_time)/60} mins.')\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
