{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import ConcatDataset, Dataset\n",
    "import torch\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA_VISIBLE_DEVICES=3 python main.py --metrics --scenario=task --replay=offline --replay_portion=1.0 --replay_config=grs --num_replay_sample=500 --iters=10000\n",
    "CUDA is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 43 26 30 37 66 87 52 90 33 22 70 47 63 65 27 11 77 25 29 48 34 69 13\n",
      "  5 96  9 40 59 45 75 32  4  1  2 73 60 99 86 95 89 62  0 18 71 51 85 78\n",
      "  8 91 80 81 42 24 88 57 55 38 17 21 23 20 58 68 46 64  6 79 31 61 84 56\n",
      " 49 39 67 93 41 15 98  3 10 35  7 83 28 54 76 82 97 19 16 53 50 14 74 72\n",
      " 44 92 94 36]\n",
      " Training data X (303331, 2381) Y (303331,)\n",
      " Test data X (33704, 2381) Y (33704,)\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54], [55, 56, 57, 58, 59], [60, 61, 62, 63, 64], [65, 66, 67, 68, 69], [70, 71, 72, 73, 74], [75, 76, 77, 78, 79], [80, 81, 82, 83, 84], [85, 86, 87, 88, 89], [90, 91, 92, 93, 94], [95, 96, 97, 98, 99]]\n"
     ]
    }
   ],
   "source": [
    "def get_selected_classes(target_classes):\n",
    "    classes_Y = [i for i in range(100)]\n",
    "    #print(classes_Y)\n",
    "    selected_classes = np.random.choice(classes_Y, target_classes,replace=False)\n",
    "    #print(selected_classes)\n",
    "    \n",
    "    return selected_classes\n",
    "\n",
    "def V2_get_continual_ember_class_data(data_dir, train=True):\n",
    "    \n",
    "    if train:\n",
    "        data_dir = data_dir + '/'\n",
    "        XY_train = np.load(data_dir + 'XY_train.npz')\n",
    "        X_tr, Y_tr = XY_train['X_train'], XY_train['Y_train']\n",
    "\n",
    "        return X_tr, Y_tr\n",
    "    else:\n",
    "        data_dir = data_dir + '/'\n",
    "        XY_test = np.load(data_dir + 'XY_test.npz')\n",
    "        X_test, Y_test = XY_test['X_test'], XY_test['Y_test']\n",
    "\n",
    "        return X_test, Y_test \n",
    "\n",
    "\n",
    "def get_ember_selected_class_data(data_dir, selected_classes, train=True):\n",
    "    \n",
    "    \n",
    "    if train:\n",
    "        all_X, all_Y = V2_get_continual_ember_class_data(data_dir, train=True)\n",
    "    else:\n",
    "        all_X, all_Y = V2_get_continual_ember_class_data(data_dir, train=False)\n",
    "    \n",
    "    X_ = []\n",
    "    Y_ = []\n",
    "\n",
    "    for ind, cls in enumerate(selected_classes):\n",
    "        get_ind_cls = np.where(all_Y == cls)\n",
    "        cls_X = all_X[get_ind_cls]\n",
    "        #cls_Y = all_Y[get_ind_cls]\n",
    "\n",
    "        #assert len(cls_Y) == len(cls_X)\n",
    "\n",
    "        for j in range(len(cls_X)):\n",
    "            X_.append(cls_X[j])\n",
    "            Y_.append(ind)\n",
    "\n",
    "    #from sklearn.utils import shuffle        \n",
    "    X_ = np.float32(np.array(X_))\n",
    "    Y_ = np.array(Y_, dtype=np.int64)\n",
    "    X_, Y_ = shuffle(X_, Y_)\n",
    "\n",
    "    if train:\n",
    "        print(f' Training data X {X_.shape} Y {Y_.shape}')\n",
    "    else:\n",
    "        print(f' Test data X {X_.shape} Y {Y_.shape}')\n",
    "    \n",
    "    return X_, Y_\n",
    "\n",
    "\n",
    "class malwareSubDataset(Dataset):\n",
    "    '''To sub-sample a dataset, taking only those samples with label in [sub_labels].\n",
    "\n",
    "    After this selection of samples has been made, it is possible to transform the target-labels,\n",
    "    which can be useful when doing continual learning with fixed number of output units.'''\n",
    "    \n",
    "    def __init__(self, original_dataset, orig_length_features,\\\n",
    "                 target_length_features, sub_labels):\n",
    "        super().__init__()\n",
    "        #print(target_transform)\n",
    "        self.dataset, self.origlabels = original_dataset\n",
    "        self.orig_length_features = orig_length_features\n",
    "        self.target_length_features = target_length_features\n",
    "        \n",
    "        self.sub_indeces = []\n",
    "        for index in range(len(self.dataset)):\n",
    "            label = self.origlabels[index]\n",
    "            \n",
    "            if label in sub_labels:\n",
    "                self.sub_indeces.append(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sub_indeces)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        self.padded_features = np.zeros(self.target_length_features - self.orig_length_features, dtype=np.float32)\n",
    "        sample = np.concatenate((self.dataset[self.sub_indeces[index]],self.padded_features))\n",
    "        target = self.origlabels[self.sub_indeces[index]]\n",
    "        \n",
    "        return (sample, target)  \n",
    "\n",
    "\n",
    "def get_malware_multitask_experiment(target_classes,\\\n",
    "                                     orig_feats_length, target_feats_length,\\\n",
    "                                     scenario, tasks,\\\n",
    "                                     data_dir=\"../../../ember2018/top_class_bases/top_classes_100\"):\n",
    "    \n",
    "    num_class = target_classes\n",
    "\n",
    "    classes_per_task = 5\n",
    "\n",
    "    selected_classes = get_selected_classes(target_classes)\n",
    "\n",
    "    print(selected_classes)\n",
    "\n",
    "    X_train, Y_train = get_ember_selected_class_data(data_dir, selected_classes, train=True)\n",
    "    X_test, Y_test = get_ember_selected_class_data(data_dir, selected_classes, train=False)\n",
    "\n",
    "    \n",
    "    standardization = StandardScaler()\n",
    "    standard_scaler = standardization.fit(X_train)\n",
    "    X_train = standard_scaler.transform(X_train)\n",
    "    X_test = standard_scaler.transform(X_test)  \n",
    "\n",
    "    ember_train, ember_test = (X_train, Y_train), (X_test, Y_test)\n",
    "\n",
    "\n",
    "    first_task = list(range(50)) #[0, 1, .., 49]\n",
    "\n",
    "    labels_per_task = list([first_task]) + [ \n",
    "        list(np.array(range(classes_per_task)) + classes_per_task * task_id) for task_id in range(10,20)]\n",
    "\n",
    "    print(labels_per_task)\n",
    "\n",
    "#     # split them up into sub-tasks\n",
    "#     train_datasets = []\n",
    "#     test_datasets = []\n",
    "#     for labels in labels_per_task:\n",
    "#         train_datasets.append(malwareSubDataset(ember_train, orig_feats_length,\\\n",
    "#                                                 target_feats_length, labels))\n",
    "#         test_datasets.append(malwareSubDataset(ember_test, orig_feats_length,\\\n",
    "#                                                target_feats_length, labels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Return tuple of train-, validation- and test-dataset, and number of classes per task\n",
    "    #return ((train_datasets, test_datasets), classes_per_task, labels_per_task)\n",
    "    return (ember_train, ember_test, classes_per_task)\n",
    "\n",
    "\n",
    "target_classes = 100\n",
    "scenario = 'class'\n",
    "orig_feats_length, target_feats_length = 2381, 2381\n",
    "tasks = 11\n",
    "\n",
    "# (train_datasets, test_datasets), classes_per_task, (ember_test, labels_per_task) = get_malware_multitask_experiment(\n",
    "#         target_classes=target_classes, scenario=scenario,\\\n",
    "#         orig_feats_length=orig_feats_length,\\\n",
    "#         target_feats_length=target_feats_length, tasks=tasks\n",
    "#     )\n",
    "\n",
    "\n",
    "ember_train, ember_test, classes_per_task  = get_malware_multitask_experiment(\n",
    "        target_classes=target_classes, scenario=scenario,\\\n",
    "        orig_feats_length=orig_feats_length,\\\n",
    "        target_feats_length=target_feats_length, tasks=tasks\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partial_data(X, Y, replay_portion):\n",
    "    indx = [i for i in range(len(Y))]\n",
    "    random.shuffle(indx)\n",
    "\n",
    "    replay_data_size = int(len(indx)*replay_portion)\n",
    "    replay_index = indx[:replay_data_size]\n",
    "\n",
    "    X_train = X[replay_index]\n",
    "    Y_train = Y[replay_index]\n",
    "    \n",
    "    return X_train, Y_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_class_grs(PreviousTasksData, PreviousTasksLabels,\\\n",
    "                  replay_portion=0.5):\n",
    "    #replay_portion = 0.5\n",
    "    \n",
    "    X, Y = PreviousTasksData\n",
    "    \n",
    "    all_replay_X = []\n",
    "    all_replay_Y = []\n",
    "    for previousTask, CurrentTaskLabels in enumerate(PreviousTasksLabels):\n",
    "        for task_Y in CurrentTaskLabels:\n",
    "            Y_task_ind = np.where(Y == task_Y)\n",
    "\n",
    "            task_samples = X[Y_task_ind]\n",
    "            task_labels = Y[Y_task_ind]\n",
    "\n",
    "            for ind, l in enumerate(task_labels):\n",
    "                all_replay_X.append(task_samples[ind])\n",
    "                all_replay_Y.append(l)\n",
    "\n",
    "\n",
    "    all_replay_X, all_replay_Y = np.array(all_replay_X), np.array(all_replay_Y)\n",
    "    unique_labels = np.unique(all_replay_Y)\n",
    "    \n",
    "    #print(f'all_replay_X {all_replay_X.shape} all_replay_Y {all_replay_Y.shape}')\n",
    "    all_replay_X, all_replay_Y = get_partial_data(all_replay_X, all_replay_Y, replay_portion)\n",
    "    #print(f'all_replay_X {all_replay_X.shape} all_replay_Y {all_replay_Y.shape}')\n",
    "\n",
    "    #print(unique_labels)\n",
    "    \n",
    "    return all_replay_X, all_replay_Y\n",
    "\n",
    "def get_current_task_data(CurrentTaskData, CurrentTaskLabels):\n",
    "    \n",
    "    X, Y = CurrentTaskData\n",
    "    \n",
    "    X_task_samples = []\n",
    "    Y_task_labels = []\n",
    "    \n",
    "    for task_Y in CurrentTaskLabels:\n",
    "        Y_task_ind = np.where(Y == task_Y)\n",
    "        \n",
    "        task_samples = X[Y_task_ind]\n",
    "        task_labels = Y[Y_task_ind]\n",
    "        \n",
    "        for ind, l in enumerate(task_labels):\n",
    "            X_task_samples.append(task_samples[ind])\n",
    "            Y_task_labels.append(l)\n",
    "        \n",
    "    X_task_samples, Y_task_labels = np.array(X_task_samples),\\\n",
    "                                    np.array(Y_task_labels)\n",
    "    \n",
    "    return X_task_samples, Y_task_labels\n",
    "\n",
    "\n",
    "\n",
    "class malwareTrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.samples, self.labels = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = self.samples[index]\n",
    "        target = self.labels[index]\n",
    "        \n",
    "        return (sample, target) \n",
    "    \n",
    "\n",
    "    \n",
    "diversity_mode = 'grs' #'ifs' #ahrs \n",
    "\n",
    "\n",
    "first_task = list(range(50)) #[0, 1, .., 49]\n",
    "\n",
    "labels_per_task = list([first_task]) + [ \n",
    "    list(np.array(range(classes_per_task)) + classes_per_task * task_id) for task_id in range(10,20)]\n",
    "\n",
    "#print(labels_per_task)\n",
    "    \n",
    "# ember_train, ember_test    \n",
    "    \n",
    "# all_current_X, all_current_Y = get_current_task_data(ember_train, labels_per_task[4])\n",
    "\n",
    "all_replay_X, all_replay_Y = get_class_grs(ember_train, labels_per_task[2:3])\n",
    "\n",
    "\n",
    "# all_X = np.concatenate((all_current_X, all_replay_X))\n",
    "# all_Y = np.concatenate((all_current_Y, all_replay_Y))\n",
    "\n",
    "# print(np.unique(all_Y))\n",
    "\n",
    "# task_train_dataset = malwareTrainDataset((all_X, all_Y))\n",
    "\n",
    "\n",
    "# cnt = 0\n",
    "# labels_ = []\n",
    "# for a, b in task_train_dataset:\n",
    "    \n",
    "#     cnt += 1\n",
    "    \n",
    "#     #print(a)\n",
    "#     labels_.append(b)\n",
    "# #     if cnt == 10:\n",
    "# #         break\n",
    "# print(np.unique(labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55, 56, 57, 58, 59])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_replay_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_replay_X (101873, 2381) all_replay_Y (101873,)\n"
     ]
    }
   ],
   "source": [
    "def get_partial_data(X, Y, replay_portion):\n",
    "    indx = [i for i in range(len(Y))]\n",
    "    random.shuffle(indx)\n",
    "\n",
    "    replay_data_size = int(len(indx)*replay_portion)\n",
    "    replay_index = indx[:replay_data_size]\n",
    "\n",
    "    X_train = X[replay_index]\n",
    "    Y_train = Y[replay_index]\n",
    "    \n",
    "    return X_train, Y_train\n",
    "\n",
    "def get_class_grs(PreviousTasksData, PreviousTasksLabels,\\\n",
    "                  replay_portion=0.5):\n",
    "    #replay_portion = 0.5\n",
    "    \n",
    "    X, Y = PreviousTasksData\n",
    "    \n",
    "    all_replay_X = []\n",
    "    all_replay_Y = []\n",
    "    for previousTask, CurrentTaskLabels in enumerate(PreviousTasksLabels):\n",
    "        for task_Y in CurrentTaskLabels:\n",
    "            Y_task_ind = np.where(Y == task_Y)\n",
    "\n",
    "            task_samples = X[Y_task_ind]\n",
    "            task_labels = Y[Y_task_ind]\n",
    "\n",
    "            for ind, l in enumerate(task_labels):\n",
    "                all_replay_X.append(task_samples[ind])\n",
    "                all_replay_Y.append(l)\n",
    "\n",
    "\n",
    "    all_replay_X, all_replay_Y = np.array(all_replay_X), np.array(all_replay_Y)\n",
    "    unique_labels = np.unique(all_replay_Y)\n",
    "    \n",
    "    #print(f'all_replay_X {all_replay_X.shape} all_replay_Y {all_replay_Y.shape}')\n",
    "    all_replay_X, all_replay_Y = get_partial_data(all_replay_X, all_replay_Y, replay_portion)\n",
    "    #print(f'all_replay_X {all_replay_X.shape} all_replay_Y {all_replay_Y.shape}')\n",
    "\n",
    "    #print(unique_labels)\n",
    "    print(f'all_replay_X {all_replay_X.shape} all_replay_Y {all_replay_Y.shape}')\n",
    "    \n",
    "    return all_replay_X, all_replay_Y\n",
    "\n",
    "\n",
    "\n",
    "all_replay_X, all_replay_Y = get_class_grs(ember_train, labels_per_task[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def get_data_loader(dataset, batch_size, cuda=False, collate_fn=None, drop_last=False, augment=False):\n",
    "    '''Return <DataLoader>-object for the provided <DataSet>-object [dataset].'''\n",
    "    \n",
    "    \n",
    "    dataset_ = copy.deepcopy(dataset)\n",
    "        \n",
    "        \n",
    "    lbls = []\n",
    "    for ind, i in dataset_:\n",
    "        #print(i)\n",
    "        lbls.append(i)\n",
    "    #print(np.unique(lbls))\n",
    "    y = np.array(lbls,dtype=int)\n",
    "    class_sample_count = np.array([len(np.where(y == t)[0]) for t in np.unique(y)])\n",
    "    \n",
    "    weight = 1. / class_sample_count\n",
    "    \n",
    "    #print(class_sample_count, weight)\n",
    "        \n",
    "    new_samples_weight = []\n",
    "    unique_labels = np.unique(y)\n",
    "    \n",
    "    for lbl in y:\n",
    "        for ul_ind, ul in enumerate(unique_labels):\n",
    "            if lbl == ul:\n",
    "                new_samples_weight.append(weight[ul_ind])\n",
    "    samples_weight = np.array(new_samples_weight)\n",
    "\n",
    "    #print(weight, np.unique(samples_weight), samples_weight[:5], y[:5])\n",
    "\n",
    "    samples_weight = torch.from_numpy(samples_weight).float()\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "        \n",
    "        \n",
    "    # Create and return the <DataLoader>-object\n",
    "    return DataLoader(\n",
    "        dataset_, batch_size=batch_size, shuffle=False,\n",
    "        collate_fn=(collate_fn or default_collate), drop_last=drop_last, sampler=sampler,\n",
    "        **({'num_workers': 0, 'pin_memory': True} if cuda else {})\n",
    "    )\n",
    "\n",
    "all_current_X, all_current_Y = get_current_task_data(ember_train, labels_per_task[0])\n",
    "task_train_dataset = malwareTrainDataset((all_current_X, all_current_Y))\n",
    "data_loader = iter(get_data_loader(task_train_dataset, 128, drop_last=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "\n",
      "replay 0 previous tasks, [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]]\n",
      "current task 2 [50, 51, 52, 53, 54]\n",
      "\n",
      "replay 1 previous tasks, [[50, 51, 52, 53, 54]]\n",
      "current task 3 [55, 56, 57, 58, 59]\n",
      "\n",
      "replay 2 previous tasks, [[55, 56, 57, 58, 59]]\n",
      "current task 4 [60, 61, 62, 63, 64]\n",
      "\n",
      "replay 3 previous tasks, [[60, 61, 62, 63, 64]]\n",
      "current task 5 [65, 66, 67, 68, 69]\n",
      "\n",
      "replay 4 previous tasks, [[65, 66, 67, 68, 69]]\n",
      "current task 6 [70, 71, 72, 73, 74]\n",
      "\n",
      "replay 5 previous tasks, [[70, 71, 72, 73, 74]]\n",
      "current task 7 [75, 76, 77, 78, 79]\n",
      "\n",
      "replay 6 previous tasks, [[75, 76, 77, 78, 79]]\n",
      "current task 8 [80, 81, 82, 83, 84]\n",
      "\n",
      "replay 7 previous tasks, [[80, 81, 82, 83, 84]]\n",
      "current task 9 [85, 86, 87, 88, 89]\n",
      "\n",
      "replay 8 previous tasks, [[85, 86, 87, 88, 89]]\n",
      "current task 10 [90, 91, 92, 93, 94]\n",
      "\n",
      "replay 9 previous tasks, [[90, 91, 92, 93, 94]]\n",
      "current task 11 [95, 96, 97, 98, 99]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "first_task = list(range(50)) #[0, 1, .., 49]\n",
    "\n",
    "labels_per_task = list([first_task]) + [ \n",
    "    list(np.array(range(classes_per_task)) + classes_per_task * task_id) for task_id in range(10,20)]\n",
    "\n",
    "for task, per_task in enumerate(labels_per_task, 1):\n",
    "    if task != 1:\n",
    "        print(f'replay {task-2} previous tasks, {labels_per_task[task-2:task-1]}')\n",
    "        print(f'current task {task} {labels_per_task[task-1]}')\n",
    "        print()\n",
    "    else:\n",
    "        print(task, labels_per_task[task-1])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.1238, -0.1224, -0.2737,  ..., -0.0177, -0.0111, -0.0100],\n",
       "         [-0.5452,  0.2735,  0.2495,  ..., -0.0177, -0.0111, -0.0100],\n",
       "         [ 0.8810, -0.4711, -0.4131,  ..., -0.0177, -0.0111, -0.0100],\n",
       "         ...,\n",
       "         [ 3.6175, -0.7059, -0.6741,  ..., -0.0177, -0.0111, -0.0100],\n",
       "         [-0.3122,  1.4783,  6.3810,  ..., -0.0177, -0.0111, -0.0100],\n",
       "         [ 1.1617, -0.5020, -0.4931,  ..., -0.0177, -0.0111, -0.0100]]),\n",
       " tensor([24, 37, 29, 21, 31,  1, 38, 45, 41, 11, 41, 20, 39, 39, 34, 36, 29,  9,\n",
       "         28, 30, 16,  2, 43, 11, 17, 39, 39,  2, 30, 43, 14,  5, 17, 41,  1, 45,\n",
       "          0, 29, 21, 35,  4, 22, 39, 35, 33,  5,  4, 27, 49,  6, 48, 21, 31,  5,\n",
       "         42, 49, 15, 47, 44, 41,  5, 39, 32,  0, 27, 26, 37, 22, 23, 27, 17,  4,\n",
       "         39,  8, 47, 10, 48, 30, 30, 39, 44, 30, 37,  8, 44, 44, 49, 31, 20, 49,\n",
       "         29, 10, 17,  5,  1, 43, 34, 35, 43, 41, 40, 44, 26,  2, 29, 17, 41, 45,\n",
       "         41, 41, 45, 19, 18,  8, 23, 35, 13, 28, 10,  8, 42, 19, 16, 35,  1, 26,\n",
       "         20,  6])]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "first_task = list(range(50)) #[0, 1, .., 49]\n",
    "\n",
    "labels_per_task = list([first_task]) + [ \n",
    "    list(np.array(range(classes_per_task)) + classes_per_task * task_id) for task_id in range(10,20)]\n",
    "\n",
    "#print(labels_per_task)\n",
    "    \n",
    "# ember_train, ember_test    \n",
    "    \n",
    "all_current_X, all_current_Y = get_current_task_data(ember_train, labels_per_task[4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15874, 2381), (15874,), array([ True,  True,  True,  True,  True]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_current_X.shape, all_current_Y.shape, np.unique(all_current_Y)==labels_per_task[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 1\n",
      "task 2\n",
      "task 3\n",
      "task 4\n",
      "task 5\n",
      "task 6\n",
      "task 7\n",
      "task 8\n",
      "task 9\n",
      "task 10\n",
      "task 11\n"
     ]
    }
   ],
   "source": [
    "def get_partial_data(X, Y, replay_portion):\n",
    "    indx = [i for i in range(len(Y))]\n",
    "    random.shuffle(indx)\n",
    "\n",
    "    replay_data_size = int(len(indx)*replay_portion)\n",
    "    replay_index = indx[:replay_data_size]\n",
    "\n",
    "    X_train = X[replay_index]\n",
    "    Y_train = Y[replay_index]\n",
    "    \n",
    "    return X_train, Y_train\n",
    "\n",
    "\n",
    "num_samples_per_class = 200\n",
    "\n",
    "diversity_mode = 'random' #'ifs' #ahrs \n",
    "#family_based == None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for task, train_data in enumerate(train_datasets, 1):\n",
    "    print(f'task {task}')\n",
    "    cnt = 0\n",
    "    #print(task, train_data)\n",
    "    Ys = []\n",
    "    Xs = []\n",
    "    for x, y in train_data:\n",
    "        #print(type(xy))\n",
    "        Ys.append(y)\n",
    "        Xs.append(x)\n",
    "        \n",
    "    #print(len(np.unique(Ys)), len(Xs))\n",
    "    \n",
    "    Xs, Ys = np.array(Xs), np.array(Ys)\n",
    "    unique_labels = np.unique(Ys)\n",
    "    #print(type(Ys), type(Xs))\n",
    "#     if diversity_mode == 'random' and not family_based:\n",
    "#         for dY in unique_labels:\n",
    "#             dY_ind = np.where(Ys == dY)\n",
    "#             #print(len(dYs[0]))\n",
    "#             #print(Ys)\n",
    "#             #print(type(dY_ind), dY_ind)\n",
    "#             dYs = Ys[dY_ind]\n",
    "#             dXs = Xs[dY_ind]\n",
    "#             #print(len(dYs) == len(dXs))\n",
    "            \n",
    "            \n",
    "            \n",
    "#         pass\n",
    "    \n",
    "    #print(unique_labels)\n",
    "    \n",
    "    \n",
    "#         if cnt == 10:\n",
    "#             break\n",
    "#         cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar_samples_pool 5826 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1292 remaining_samples_to_pick 56\n",
      "anomalous_samples 144 similar_samples 56\n",
      "Num replay samples 200\n",
      "similar_samples_pool 2421 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1950 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1405 remaining_samples_to_pick 43\n",
      "anomalous_samples 157 similar_samples 43\n",
      "Num replay samples 200\n",
      "similar_samples_pool 640 remaining_samples_to_pick 129\n",
      "anomalous_samples 71 similar_samples 129\n",
      "Num replay samples 200\n",
      "similar_samples_pool 423 remaining_samples_to_pick 153\n",
      "anomalous_samples 47 similar_samples 153\n",
      "Num replay samples 200\n",
      "similar_samples_pool 973 remaining_samples_to_pick 92\n",
      "anomalous_samples 108 similar_samples 92\n",
      "Num replay samples 200\n",
      "similar_samples_pool 408 remaining_samples_to_pick 154\n",
      "anomalous_samples 46 similar_samples 154\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1585 remaining_samples_to_pick 24\n",
      "anomalous_samples 176 similar_samples 24\n",
      "Num replay samples 200\n",
      "similar_samples_pool 2737 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 594 remaining_samples_to_pick 134\n",
      "anomalous_samples 66 similar_samples 134\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1159 remaining_samples_to_pick 71\n",
      "anomalous_samples 129 similar_samples 71\n",
      "Num replay samples 200\n",
      "similar_samples_pool 733 remaining_samples_to_pick 118\n",
      "anomalous_samples 82 similar_samples 118\n",
      "Num replay samples 200\n",
      "similar_samples_pool 718 remaining_samples_to_pick 120\n",
      "anomalous_samples 80 similar_samples 120\n",
      "Num replay samples 200\n",
      "similar_samples_pool 2407 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 6645 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 497 remaining_samples_to_pick 144\n",
      "anomalous_samples 56 similar_samples 144\n",
      "Num replay samples 200\n",
      "similar_samples_pool 2522 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1965 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1072 remaining_samples_to_pick 80\n",
      "anomalous_samples 120 similar_samples 80\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1520 remaining_samples_to_pick 31\n",
      "anomalous_samples 169 similar_samples 31\n",
      "Num replay samples 200\n",
      "similar_samples_pool 599 remaining_samples_to_pick 133\n",
      "anomalous_samples 67 similar_samples 133\n",
      "Num replay samples 200\n",
      "similar_samples_pool 5166 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 11431 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 359 remaining_samples_to_pick 160\n",
      "anomalous_samples 40 similar_samples 160\n",
      "Num replay samples 200\n",
      "similar_samples_pool 7226 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1357 remaining_samples_to_pick 49\n",
      "anomalous_samples 151 similar_samples 49\n",
      "Num replay samples 200\n",
      "similar_samples_pool 845 remaining_samples_to_pick 106\n",
      "anomalous_samples 94 similar_samples 106\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1236 remaining_samples_to_pick 62\n",
      "anomalous_samples 138 similar_samples 62\n",
      "Num replay samples 200\n",
      "similar_samples_pool 514 remaining_samples_to_pick 143\n",
      "anomalous_samples 57 similar_samples 143\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1591 remaining_samples_to_pick 23\n",
      "anomalous_samples 177 similar_samples 23\n",
      "Num replay samples 200\n",
      "similar_samples_pool 13510 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 19541 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 16639 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 543 remaining_samples_to_pick 139\n",
      "anomalous_samples 61 similar_samples 139\n",
      "Num replay samples 200\n",
      "similar_samples_pool 828 remaining_samples_to_pick 108\n",
      "anomalous_samples 92 similar_samples 108\n",
      "Num replay samples 200\n",
      "similar_samples_pool 352 remaining_samples_to_pick 160\n",
      "anomalous_samples 40 similar_samples 160\n",
      "Num replay samples 200\n",
      "similar_samples_pool 450 remaining_samples_to_pick 150\n",
      "anomalous_samples 50 similar_samples 150\n",
      "Num replay samples 200\n",
      "similar_samples_pool 365 remaining_samples_to_pick 159\n",
      "anomalous_samples 41 similar_samples 159\n",
      "Num replay samples 200\n",
      "similar_samples_pool 414 remaining_samples_to_pick 154\n",
      "anomalous_samples 46 similar_samples 154\n",
      "Num replay samples 200\n",
      "similar_samples_pool 784 remaining_samples_to_pick 113\n",
      "anomalous_samples 87 similar_samples 113\n",
      "Num replay samples 200\n",
      "similar_samples_pool 29155 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 3649 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 577 remaining_samples_to_pick 135\n",
      "anomalous_samples 65 similar_samples 135\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1006 remaining_samples_to_pick 88\n",
      "anomalous_samples 112 similar_samples 88\n",
      "Num replay samples 200\n",
      "similar_samples_pool 454 remaining_samples_to_pick 149\n",
      "anomalous_samples 51 similar_samples 149\n",
      "Num replay samples 200\n",
      "similar_samples_pool 487 remaining_samples_to_pick 146\n",
      "anomalous_samples 54 similar_samples 146\n",
      "Num replay samples 200\n",
      "similar_samples_pool 8870 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 397 remaining_samples_to_pick 156\n",
      "anomalous_samples 44 similar_samples 156\n",
      "Num replay samples 200\n",
      "all_current_X (5825, 2381) all_replay_X (10000, 2381)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import random\n",
    "\n",
    "def get_IFBased_samples(family_name, family_data,\\\n",
    "                        contamination,\\\n",
    "                        num_samples_per_malware_family):\n",
    "    \n",
    "    data_X = np.array(family_data)\n",
    "    \n",
    "    if len(data_X) > num_samples_per_malware_family:\n",
    "        \n",
    "        # fit the model\n",
    "        clf = IsolationForest(max_samples=len(data_X), contamination=contamination)\n",
    "        clf.fit(data_X)\n",
    "        #scores_prediction = clf.decision_function(data_X)\n",
    "        y_pred = clf.predict(data_X)\n",
    "\n",
    "\n",
    "        anomalous_idx = np.where(y_pred == -1.0)\n",
    "        similar_idx = np.where(y_pred == 1.0)\n",
    "\n",
    "        #print(f'{family_name}: all-{len(y_pred)} anomalous-{len(anomalous_idx[0])} similar-{len(similar_idx[0])}')\n",
    "        assert len(anomalous_idx[0]) + len(similar_idx[0]) == len(y_pred)\n",
    "\n",
    "        anomalous_samples = data_X[anomalous_idx]\n",
    "        \n",
    "        if len(anomalous_samples) >= num_samples_per_malware_family:\n",
    "            anomalous_samples_pool = list(anomalous_samples)\n",
    "            remaining_samples_to_pick = int(num_samples_per_malware_family/2)\n",
    "            anomalous_samples = random.sample(anomalous_samples_pool, remaining_samples_to_pick)\n",
    "\n",
    "        else:\n",
    "            remaining_samples_to_pick = num_samples_per_malware_family - len(anomalous_samples)\n",
    "            \n",
    "        \n",
    "        if remaining_samples_to_pick <= len(similar_idx):\n",
    "            similar_samples = data_X[similar_idx]\n",
    "        else:\n",
    "            similar_samples_pool = list(data_X[similar_idx])\n",
    "            \n",
    "            print(f'similar_samples_pool {len(similar_samples_pool)} remaining_samples_to_pick {remaining_samples_to_pick}')\n",
    "            similar_samples = random.sample(similar_samples_pool, remaining_samples_to_pick)\n",
    "            \n",
    "        print(f'anomalous_samples {len(anomalous_samples)} similar_samples {len(similar_samples)}')\n",
    "        replay_samples = np.concatenate((anomalous_samples, similar_samples))\n",
    "    else:\n",
    "        replay_samples = data_X\n",
    "        \n",
    "    print(f'Num replay samples {len(replay_samples)}')\n",
    "    return replay_samples\n",
    "\n",
    "\n",
    "\n",
    "def get_class_grs(PreviousTasksData, PreviousTasksLabels, replay_config = 'ifs'):\n",
    "    #replay_portion = 0.5\n",
    "    \n",
    "    replay_portion = 0.5 #args.replay_portion\n",
    "    \n",
    "    \n",
    "    X, Y = PreviousTasksData\n",
    "    \n",
    "    if replay_config == 'frs':\n",
    "        all_replay_X = []\n",
    "        all_replay_Y = []\n",
    "        for previousTask, CurrentTaskLabels in enumerate(PreviousTasksLabels):\n",
    "            for task_Y in CurrentTaskLabels:\n",
    "                Y_task_ind = np.where(Y == task_Y)\n",
    "\n",
    "                task_samples = X[Y_task_ind]\n",
    "                task_labels = Y[Y_task_ind]\n",
    "                \n",
    "                for ind, l in enumerate(task_labels):\n",
    "                    all_replay_X.append(task_samples[ind])\n",
    "                    all_replay_Y.append(l)\n",
    "\n",
    "\n",
    "        all_replay_X, all_replay_Y = np.array(all_replay_X), np.array(all_replay_Y)\n",
    "        unique_labels = np.unique(all_replay_Y)\n",
    "        \n",
    "    elif replay_config == 'ifs':\n",
    "        all_replay_X = []\n",
    "        all_replay_Y = []\n",
    "        for previousTask, CurrentTaskLabels in enumerate(PreviousTasksLabels):\n",
    "            for task_Y in CurrentTaskLabels:\n",
    "                Y_task_ind = np.where(Y == task_Y)\n",
    "\n",
    "                task_samples = X[Y_task_ind]\n",
    "                task_labels = Y[Y_task_ind]\n",
    "                \n",
    "                task_samples = get_IFBased_samples(task_Y, task_samples,\\\n",
    "                        0.1,\\\n",
    "                        200)\n",
    "                \n",
    "                \n",
    "                for ind, ifs_sample in enumerate(task_samples):\n",
    "                    all_replay_X.append(ifs_sample)\n",
    "                    all_replay_Y.append(task_Y)\n",
    "\n",
    "\n",
    "        all_replay_X, all_replay_Y = np.array(all_replay_X), np.array(all_replay_Y)\n",
    "        unique_labels = np.unique(all_replay_Y)\n",
    "        \n",
    "        return all_replay_X, all_replay_Y\n",
    "    else:\n",
    "        all_replay_X = []\n",
    "        all_replay_Y = []\n",
    "        for previousTask, CurrentTaskLabels in enumerate(PreviousTasksLabels):\n",
    "            for task_Y in CurrentTaskLabels:\n",
    "                Y_task_ind = np.where(Y == task_Y)\n",
    "\n",
    "                task_samples = X[Y_task_ind]\n",
    "                task_labels = Y[Y_task_ind]\n",
    "\n",
    "                for ind, l in enumerate(task_labels):\n",
    "                    all_replay_X.append(task_samples[ind])\n",
    "                    all_replay_Y.append(l)\n",
    "\n",
    "\n",
    "        all_replay_X, all_replay_Y = np.array(all_replay_X), np.array(all_replay_Y)\n",
    "        unique_labels = np.unique(all_replay_Y)\n",
    "    \n",
    "        if replay_portion == 1.0:\n",
    "            return all_replay_X, all_replay_Y\n",
    "        else:\n",
    "            all_replay_X, all_replay_Y = get_partial_data(all_replay_X, all_replay_Y, replay_portion)\n",
    "            #print(f'all_replay_X {all_replay_X.shape} all_replay_Y {all_replay_Y.shape}')\n",
    "            #print(unique_labels)\n",
    "            return all_replay_X, all_replay_Y\n",
    "        \n",
    "\n",
    "        \n",
    "def get_current_task_test_data(X, Y, CurrentTaskLabels):\n",
    "    \n",
    "    X_task_samples = []\n",
    "    Y_task_labels = []\n",
    "    \n",
    "    for task_Y in CurrentTaskLabels:\n",
    "        Y_task_ind = np.where(Y == task_Y)\n",
    "        \n",
    "        task_samples = X[Y_task_ind]\n",
    "        task_labels = Y[Y_task_ind]\n",
    "        \n",
    "        for ind, l in enumerate(task_labels):\n",
    "            X_task_samples.append(task_samples[ind])\n",
    "            Y_task_labels.append(l)\n",
    "        \n",
    "    X_task_samples, Y_task_labels = np.array(X_task_samples),\\\n",
    "                                    np.array(Y_task_labels)\n",
    "    \n",
    "    return X_task_samples, Y_task_label        \n",
    "        \n",
    "init_classes = 50\n",
    "target_classes = 100\n",
    "num_class = target_classes\n",
    "\n",
    "scenario = 'class'\n",
    "\n",
    "if scenario == 'class':\n",
    "    initial_task_num_classes = init_classes\n",
    "    if initial_task_num_classes > target_classes:\n",
    "        raise ValueError(f\"Initial Number of Classes cannot be more than {target_classes} classes!\")\n",
    "    left_tasks = 11 - 1 \n",
    "    classes_per_task_except_first_task = int((num_class - initial_task_num_classes) / left_tasks)\n",
    "\n",
    "    #print(selected_classes)\n",
    "    first_task = list(range(initial_task_num_classes))\n",
    "\n",
    "    labels_per_task = [first_task] + [list(initial_task_num_classes +\\\n",
    "                                       np.array(range(classes_per_task_except_first_task)) +\\\n",
    "                                       classes_per_task_except_first_task * task_id)\\\n",
    "                                      for task_id in range(left_tasks)]\n",
    "    classes_per_task = classes_per_task_except_first_task\n",
    "\n",
    "else:\n",
    "    classes_per_task = int(np.floor(num_class / tasks))\n",
    "\n",
    "    labels_per_task = [list(np.array(range(classes_per_task)) +\\\n",
    "                        classes_per_task * task_id) for task_id in range(tasks)]    \n",
    "\n",
    "labels_per_task = labels_per_task[:2]\n",
    "# Loop over all tasks.\n",
    "for task, per_task in enumerate(labels_per_task, 1):\n",
    "\n",
    "    if task != 1:\n",
    "            all_replay_X, all_replay_Y = get_class_grs(ember_train, labels_per_task[:task-1])\n",
    "            all_current_X, all_current_Y = get_current_task_data(ember_train, labels_per_task[task-1])\n",
    "            print(f'all_current_X {all_current_X.shape} all_replay_X {all_replay_X.shape}')\n",
    "            all_X = np.concatenate((all_current_X, all_replay_X))\n",
    "            all_Y = np.concatenate((all_current_Y, all_replay_Y))\n",
    "    else:\n",
    "        all_X, all_Y = get_current_task_data(ember_train, labels_per_task[task-1])\n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"../../../ember2018/top_class_bases/top_classes_100/\"\n",
    "XY_train = np.load(data_dir + 'XY_train.npz')\n",
    "Y_tr = XY_train['Y_train']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ys = np.unique(Y_tr)\n",
    "\n",
    "numS = []\n",
    "\n",
    "for Yi in Ys:\n",
    "    YiN = len(np.where(Y_tr == Yi)[0])\n",
    "    numS.append(YiN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(np.array(numS) >= 1000)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29], [30, 31, 32, 33, 34], [35, 36, 37, 38, 39], [40, 41, 42, 43, 44], [45, 46, 47, 48, 49], [50, 51, 52, 53, 54], [55, 56, 57, 58, 59], [60, 61, 62, 63, 64], [65, 66, 67, 68, 69], [70, 71, 72, 73, 74], [75, 76, 77, 78, 79], [80, 81, 82, 83, 84], [85, 86, 87, 88, 89], [90, 91, 92, 93, 94], [95, 96, 97, 98, 99]]\n",
      "current task 1\n",
      "current task labels [0, 1, 2, 3, 4]\n",
      "rest task labels [[5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29], [30, 31, 32, 33, 34], [35, 36, 37, 38, 39], [40, 41, 42, 43, 44], [45, 46, 47, 48, 49], [50, 51, 52, 53, 54], [55, 56, 57, 58, 59], [60, 61, 62, 63, 64], [65, 66, 67, 68, 69], [70, 71, 72, 73, 74], [75, 76, 77, 78, 79], [80, 81, 82, 83, 84], [85, 86, 87, 88, 89], [90, 91, 92, 93, 94], [95, 96, 97, 98, 99]]\n",
      "\n",
      "\n",
      " current_Y 14329\n",
      "\n",
      " all_rest_Y 289002\n",
      "\n",
      "current task 2\n",
      "similar_samples_pool 5826 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1292 remaining_samples_to_pick 56\n",
      "anomalous_samples 144 similar_samples 56\n",
      "Num replay samples 200\n",
      "similar_samples_pool 2422 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1951 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1405 remaining_samples_to_pick 43\n",
      "anomalous_samples 157 similar_samples 43\n",
      "Num replay samples 200\n",
      "prev_replay_Y (1000,) [0 1 2 3 4]\n",
      "all_replay_Y [0 1 2 3 4]\n",
      "\n",
      "\n",
      " current_Y 4477\n",
      "\n",
      " all_rest_Y 285525\n",
      "\n",
      "current task 3\n",
      "similar_samples_pool 640 remaining_samples_to_pick 129\n",
      "anomalous_samples 71 similar_samples 129\n",
      "Num replay samples 200\n",
      "similar_samples_pool 423 remaining_samples_to_pick 153\n",
      "anomalous_samples 47 similar_samples 153\n",
      "Num replay samples 200\n",
      "similar_samples_pool 973 remaining_samples_to_pick 92\n",
      "anomalous_samples 108 similar_samples 92\n",
      "Num replay samples 200\n",
      "similar_samples_pool 408 remaining_samples_to_pick 154\n",
      "anomalous_samples 46 similar_samples 154\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1585 remaining_samples_to_pick 24\n",
      "anomalous_samples 176 similar_samples 24\n",
      "Num replay samples 200\n",
      "prev_replay_Y (1000,) [5 6 7 8 9]\n",
      "all_replay_Y [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "\n",
      " current_Y 6603\n",
      "\n",
      " all_rest_Y 279922\n",
      "\n",
      "current task 4\n",
      "similar_samples_pool 2737 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 594 remaining_samples_to_pick 134\n",
      "anomalous_samples 66 similar_samples 134\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1159 remaining_samples_to_pick 71\n",
      "anomalous_samples 129 similar_samples 71\n",
      "Num replay samples 200\n",
      "similar_samples_pool 733 remaining_samples_to_pick 118\n",
      "anomalous_samples 82 similar_samples 118\n",
      "Num replay samples 200\n",
      "similar_samples_pool 718 remaining_samples_to_pick 120\n",
      "anomalous_samples 80 similar_samples 120\n",
      "Num replay samples 200\n",
      "prev_replay_Y (1000,) [10 11 12 13 14]\n",
      "all_replay_Y [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "\n",
      " current_Y 15595\n",
      "\n",
      " all_rest_Y 265327\n",
      "\n",
      "current task 5\n",
      "similar_samples_pool 2407 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 6642 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 497 remaining_samples_to_pick 144\n",
      "anomalous_samples 56 similar_samples 144\n",
      "Num replay samples 200\n",
      "similar_samples_pool 2522 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "similar_samples_pool 1965 remaining_samples_to_pick 100\n",
      "anomalous_samples 100 similar_samples 100\n",
      "Num replay samples 200\n",
      "prev_replay_Y (1000,) [15 16 17 18 19]\n",
      "all_replay_Y [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "\n",
      "\n",
      " current_Y 21988\n",
      "\n",
      " all_rest_Y 244339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_current_task_rest_data(X, Y, CurrentTaskLabels):\n",
    "    \n",
    "    X_task_samples = []\n",
    "    Y_task_labels = []\n",
    "    \n",
    "    for task_Y in CurrentTaskLabels:\n",
    "        Y_task_ind = np.where(Y == task_Y)\n",
    "        \n",
    "        task_samples = X[Y_task_ind]\n",
    "        task_labels = Y[Y_task_ind]\n",
    "        \n",
    "        for ind, l in enumerate(task_labels):\n",
    "            X_task_samples.append(task_samples[ind])\n",
    "            Y_task_labels.append(l)\n",
    "        \n",
    "    X_task_samples, Y_task_labels = np.array(X_task_samples),\\\n",
    "                                    np.array(Y_task_labels)\n",
    "    \n",
    "    return X_task_samples, Y_task_labels \n",
    "\n",
    "def get_rest_task_data(RestTasksData, RestTasksLabels):\n",
    "    \n",
    "    X, Y = RestTasksData\n",
    "    \n",
    "    all_rest_X = []\n",
    "    all_rest_Y = []\n",
    "    for restTask, restCurrentTaskLabels in enumerate(RestTasksLabels):\n",
    "        for task_Y in restCurrentTaskLabels:\n",
    "            Y_task_ind = np.where(Y == task_Y)\n",
    "\n",
    "            task_samples = X[Y_task_ind]\n",
    "            task_labels = Y[Y_task_ind]\n",
    "\n",
    "            for ind, l in enumerate(task_labels):\n",
    "                all_rest_X.append(task_samples[ind])\n",
    "                all_rest_Y.append(l)\n",
    "\n",
    "\n",
    "    all_rest_X, all_rest_Y = np.array(all_rest_X), np.array(all_rest_Y)\n",
    "    #unique_labels = np.unique(all_rest_Y)\n",
    "\n",
    "    return all_rest_X, all_rest_Y\n",
    "\n",
    "\n",
    "tasks = 20\n",
    "classes_per_task = int(np.floor(num_class / tasks))\n",
    "\n",
    "labels_per_task = [list(np.array(range(classes_per_task)) +\\\n",
    "                    classes_per_task * task_id) for task_id in range(tasks)] \n",
    "\n",
    "#print(labels_per_task)\n",
    "\n",
    "replay_mode =\"offline\"\n",
    "scenario = \"task\"\n",
    "#task = 1\n",
    "\n",
    "all_replay_X, all_replay_Y = [], []\n",
    "\n",
    "for task in range(1,6):\n",
    "    if replay_mode==\"offline\" and scenario == \"task\":\n",
    "        print(f'current task {task}')\n",
    "        \n",
    "        if task == 1:\n",
    "            current_X, current_Y = get_current_task_data(ember_train, labels_per_task[task-1])\n",
    "\n",
    "            print(f'current task labels {labels_per_task[task-1]}')\n",
    "            print(f'rest task labels {labels_per_task[task:]}')\n",
    "            rest_X, rest_Y = get_rest_task_data(ember_train, labels_per_task[task:])\n",
    "            all_rest_X = rest_X\n",
    "            all_rest_Y = rest_Y\n",
    "\n",
    "        else:\n",
    "            current_X, current_Y = get_current_task_data(ember_train, labels_per_task[task-1])\n",
    "\n",
    "            prev_replay_X, prev_replay_Y = get_class_grs(ember_train, labels_per_task[task-2:task-1])\n",
    "            print(f'prev_replay_Y {prev_replay_Y.shape} {np.unique(prev_replay_Y)}')\n",
    "            \n",
    "            if task > 2:\n",
    "                all_replay_X, all_replay_Y = np.concatenate((all_replay_X, prev_replay_X)),\\\n",
    "                                                     np.concatenate((all_replay_Y, prev_replay_Y))\n",
    "            else:\n",
    "                all_replay_X, all_replay_Y = prev_replay_X, prev_replay_Y\n",
    "                \n",
    "            print(f'all_replay_Y {np.unique(all_replay_Y)}')\n",
    "            \n",
    "            \n",
    "            if task != tasks:\n",
    "                rest_X, rest_Y = get_rest_task_data(ember_train, labels_per_task[task:])\n",
    "\n",
    "                all_rest_X = np.concatenate((all_replay_X, rest_X))\n",
    "                all_rest_Y = np.concatenate((all_replay_Y, rest_Y))\n",
    "\n",
    "            else:\n",
    "                all_rest_X = all_replay_X\n",
    "                all_rest_Y = all_replay_Y\n",
    "\n",
    "        print()\n",
    "        print(f'\\n current_Y {len(current_Y)}')\n",
    "        print(f'\\n all_rest_Y {len(all_rest_Y)}\\n')\n",
    "    #     standard_scaler = standardization.partial_fit(current_X)\n",
    "\n",
    "    #     current_X = standard_scaler.transform(current_X)\n",
    "    #     all_rest_X = standard_scaler.transform(all_rest_X)\n",
    "\n",
    "        x_test, y_test = ember_test\n",
    "    #     x_test = standard_scaler.transform(x_test)\n",
    "\n",
    "\n",
    "        train_datasets = [None]*tasks\n",
    "        for ct, labels in enumerate(labels_per_task):\n",
    "            if ct == task:\n",
    "                train_dataset = malwareTrainDataset((current_X, current_Y))\n",
    "                training_dataset = train_dataset\n",
    "                train_datasets[ct] = train_dataset\n",
    "            else:\n",
    "                rest_task_X, rest_task_Y = get_current_task_rest_data(all_rest_X, all_rest_Y, labels)\n",
    "                train_datasets[ct] = malwareTrainDataset((rest_task_X, rest_task_Y))\n",
    "\n",
    "\n",
    "    previous_datasets = train_datasets\n",
    "    iter(get_data_loader(previous_datasets[task], 512, drop_last=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x7f397d012510>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(get_data_loader(previous_datasets[task], 512, drop_last=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "        22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "        39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "        56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "        73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "        90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " array([0, 1, 2, 3, 4]))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_rest_Y), np.unique(prev_replay_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-f2226c5e26a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-220cf9ce34a7>\u001b[0m in \u001b[0;36mget_data_loader\u001b[0;34m(dataset, batch_size, cuda, collate_fn, drop_last, augment)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0msamples_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWeightedRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.7/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weights, num_samples, replacement, generator)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m--> 157\u001b[0;31m                              \"value, but got num_samples={}\".format(num_samples))\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             raise ValueError(\"replacement should be a boolean value, but got \"\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "get_data_loader(previous_datasets[1], 512, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186474"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(previous_datasets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f397cf94cd0>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "if scenario==\"task\":\n",
    "\n",
    "    up_to_task = task\n",
    "    \n",
    "    batch_size_replay = int(np.ceil(batch_size/up_to_task)) if (up_to_task>1) else batch_size\n",
    "    \n",
    "    # -in Task-IL scenario, need separate replay for each task\n",
    "    for task_id in range(up_to_task):\n",
    "        batch_size_to_use = min(batch_size_replay, len(previous_datasets[task_id]))\n",
    "        \n",
    "        iters_left_previous[task_id] -= 1\n",
    "        \n",
    "        if iters_left_previous[task_id] == 0:\n",
    "            data_loader_previous[task_id] = iter(utils.get_data_loader(\n",
    "                previous_datasets[task_id], batch_size_to_use, cuda=cuda, drop_last=True\n",
    "            ))\n",
    "            iters_left_previous[task_id] = len(data_loader_previous[task_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
