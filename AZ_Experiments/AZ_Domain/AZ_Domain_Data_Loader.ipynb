{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CountV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import scipy\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time, random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "# from ember_utils import *\n",
    "# from ember_model import *\n",
    "# from ember_pjr_utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_path = '/home/mr6564/continual_research/AZ_Data/Domain/'\n",
    "years = ['2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016']\n",
    "\n",
    "all_X_tr, all_Y_tr, all_Y_tr_family = [], [], []\n",
    "all_X_te, all_Y_te, all_Y_te_family = [], [], []\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    tr_file = raw_path + year + '_Domain_AZ_Train.npz'\n",
    "    te_file = raw_path + year + '_Domain_AZ_Test.npz'\n",
    "    \n",
    "    tr_year = np.load(tr_file, allow_pickle=True)\n",
    "    te_year = np.load(te_file, allow_pickle=True)\n",
    "    \n",
    "    X_tr_year, Y_tr_year, Y_tr_family_year = tr_year['X_train'], tr_year['Y_train'], tr_year['Y_tr_family']\n",
    "    X_te_year, Y_te_year, Y_te_family_year = te_year['X_test'], te_year['Y_test'], te_year['Y_te_family']\n",
    "    \n",
    "    \n",
    "    all_X_tr, all_Y_tr, all_Y_tr_family = np.concatenate((all_X_tr, X_tr_year)),\\\n",
    "                                            np.concatenate((all_Y_tr, Y_tr_year)),\\\n",
    "                                            np.concatenate((all_Y_tr_family, Y_tr_family_year))\n",
    "    \n",
    "    all_X_te, all_Y_te, all_Y_te_family = np.concatenate((all_X_te, X_te_year)),\\\n",
    "                                            np.concatenate((all_Y_te, Y_te_year)),\\\n",
    "                                            np.concatenate((all_Y_te_family, Y_te_family_year))\n",
    "\n",
    "#print(len(np.where(Y_tr_family_year == 'goodware')[0]), len(np.where(Y_tr_year == 0)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Data shape: (682598, 3858791)\n",
      "(75848, 3858791)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_vocabulary(data):\n",
    "    \"\"\"Build a vocabulary from a list of lists of strings.\"\"\"\n",
    "    vocab_set = set(word for sample in data for word in sample)\n",
    "    return sorted(list(vocab_set))  # Sort for consistency\n",
    "\n",
    "def vectorize_samples(data, vocabulary):\n",
    "    \"\"\"Vectorize data based on the given vocabulary.\"\"\"\n",
    "    vocab_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    vectorized_data = np.zeros((len(data), len(vocabulary)), dtype=int)\n",
    "\n",
    "    for i, sample in enumerate(data):\n",
    "        for word in sample:\n",
    "            if word in vocab_index:\n",
    "                vectorized_data[i, vocab_index[word]] = 1\n",
    "\n",
    "    return vectorized_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform_with_training_vocab(training_vocab_list, data_samples):\n",
    "    \"\"\"\n",
    "    Transform the data samples using the vocabulary list from the training data.\n",
    "    :param training_vocab_list: List of words from the training data\n",
    "    :param data_samples: List of data samples (each sample is a list of words)\n",
    "    :return: Vectorized data as a NumPy array\n",
    "    \"\"\"\n",
    "    # Convert the vocabulary list to a dictionary {word: index}\n",
    "    training_vocab_dict = {word: idx for idx, word in enumerate(training_vocab_list)}\n",
    "\n",
    "    # Create a zero matrix with dimensions: number of samples x size of vocabulary\n",
    "    vectorized = np.zeros((len(data_samples), len(training_vocab_list)), dtype=int)\n",
    "\n",
    "    for i, sample in enumerate(data_samples):\n",
    "        for word in sample:\n",
    "            if word in training_vocab_dict:\n",
    "                vectorized[i, training_vocab_dict[word]] = 1\n",
    "\n",
    "    return vectorized\n",
    "\n",
    "\n",
    "\n",
    "def vectorize_samples_sparse(data, vocabulary):\n",
    "    vocab_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    vectorized_data = lil_matrix((len(data), len(vocabulary)), dtype=int)\n",
    "    for i, sample in enumerate(data):\n",
    "        for word in sample:\n",
    "            if word in vocab_index:\n",
    "                vectorized_data[i, vocab_index[word]] += 1\n",
    "    return vectorized_data.tocsr()  # Convert to CSR format for efficient arithmetic and matrix vector operations\n",
    "\n",
    "\n",
    "\n",
    "def transform_with_training_vocab_sparse(training_vocab_list, data_samples):\n",
    "    training_vocab_dict = {word: idx for idx, word in enumerate(training_vocab_list)}\n",
    "    # Create a sparse matrix instead of a dense numpy array\n",
    "    vectorized = lil_matrix((len(data_samples), len(training_vocab_list)), dtype=int)\n",
    "\n",
    "    for i, sample in enumerate(data_samples):\n",
    "        for word in sample:\n",
    "            if word in training_vocab_dict:\n",
    "                vectorized[i, training_vocab_dict[word]] += 1\n",
    "    return vectorized.tocsr()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# all_X_tr, all_Y_tr, all_Y_tr_family\n",
    "# all_X_te, all_Y_te, all_Y_te_family\n",
    "\n",
    "\n",
    "data = all_X_tr\n",
    "vocabulary = build_vocabulary(data)\n",
    "vectorized_data = vectorize_samples_sparse(data, vocabulary)\n",
    "\n",
    "print(\"Vectorized Data shape:\", vectorized_data.shape)\n",
    "\n",
    "vectorized_test_data = transform_with_training_vocab_sparse(vocabulary, all_X_te)\n",
    "print(vectorized_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758446"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "682598 + 75848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "selector.fit(vectorized_data)\n",
    "\n",
    "X_train_selected = selector.transform(vectorized_data)\n",
    "X_test_selected = selector.transform(vectorized_test_data)\n",
    "\n",
    "X_train, Y_train = X_train_selected, all_Y_tr\n",
    "X_test, Y_test = X_test_selected, all_Y_te\n",
    "\n",
    "\n",
    "print(f'after variance thresholding')\n",
    "print(X_train_selected.shape, X_test_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    tr_file = raw_path + year + '_Domain_AZ_Train.npz'\n",
    "    te_file = raw_path + year + '_Domain_AZ_Test.npz'\n",
    "    \n",
    "    tr_year = np.load(tr_file, allow_pickle=True)\n",
    "    te_year = np.load(te_file, allow_pickle=True)\n",
    "    \n",
    "    X_tr_year, Y_tr_year, Y_tr_family_year = tr_year['X_train'], tr_year['Y_train'], tr_year['Y_tr_family']\n",
    "    X_te_year, Y_te_year, Y_te_family_year = te_year['X_test'], te_year['Y_test'], te_year['Y_te_family']\n",
    "    \n",
    "    X_tr_year_vocab_vector = transform_with_training_vocab_sparse(vocabulary, X_tr_year)\n",
    "    X_te_year_vocab_vector = transform_with_training_vocab_sparse(vocabulary, X_te_year)\n",
    "    \n",
    "    X_tr_year_transformed = selector.transform(X_tr_year_vocab_vector)\n",
    "    X_te_year_transformed = selector.transform(X_te_year_vocab_vector)\n",
    "    \n",
    "    print(f'X_tr_year_vocab_vector {X_tr_year_vocab_vector.shape}\\n X_tr_year_transformed {X_tr_year_transformed.shape}')\n",
    "    print(f'X_te_year_vocab_vector {X_tr_year_vocab_vector.shape} \\n X_te_year_transformed {X_tr_year_transformed.shape}')\n",
    "    \n",
    "    \n",
    "    save_path = '/home/mr6564/continual_research/AZ_Data/Domain_Transformed/'\n",
    "    tr_file_save = save_path + year + '_Domain_AZ_Train_Transformed.npz'\n",
    "    te_file_save = save_path + year + '_Domain_AZ_Test_Transformed.npz'\n",
    "    \n",
    "    np.savez(tr_file_save, X_train=X_tr_year_transformed.toarray(), Y_train=Y_tr_year, Y_tr_family = Y_tr_family_year)\n",
    "    np.savez(te_file_save, X_test=X_te_year_transformed.toarray(), Y_test=Y_te_year, Y_te_family = Y_te_family_year)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_year = np.load(tr_file_save, allow_pickle=True)\n",
    "te_year = np.load(te_file_save, allow_pickle=True)\n",
    "\n",
    "X_tr_year, Y_tr_year_tmp, Y_tr_family_year_tmp = tr_year['X_train'], tr_year['Y_train'], tr_year['Y_tr_family']\n",
    "X_te_year, Y_te_year_tmp, Y_te_family_year_tmp = te_year['X_test'], te_year['Y_test'], te_year['Y_te_family']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ember_MLP_Net(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(Ember_MLP_Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_features, 1024)\n",
    "        self.fc1_bn = nn.BatchNorm1d(1024)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc1_drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc2_bn = nn.BatchNorm1d(512)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.fc2_drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc3_bn = nn.BatchNorm1d(256)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.fc3_drop = nn.Dropout(p=0.5)        \n",
    "        \n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc4_bn = nn.BatchNorm1d(128)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.fc4_drop = nn.Dropout(p=0.5)  \n",
    "        \n",
    "        self.fc_last = nn.Linear(128, 1) \n",
    "        self.out = nn.Sigmoid()\n",
    "        \n",
    "        #self.activate = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.act1(x) \n",
    "        x = self.fc1_drop(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2_bn(x)\n",
    "        x = self.act2(x) \n",
    "        x = self.fc2_drop(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.fc3_bn(x)\n",
    "        x = self.act3(x) \n",
    "        x = self.fc3_drop(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.fc4_bn(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.fc4_drop(x)\n",
    "        \n",
    "        x = self.fc_last(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "def testing_aucscore(model, X_test, Y_test, batch_size, device):\n",
    "    #X_te = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "    #y_te = torch.from_numpy(Y_test).type(torch.FloatTensor) \n",
    "    \n",
    "    testloader = get_dataloader(X_test, Y_test, batch_size, train_data=False)   \n",
    "    \n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    y_true_list = []\n",
    "    test_acc = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in tqdm(testloader):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_test_pred = model(x_batch)\n",
    "            tmp_test_acc = binary_acc(y_test_pred, y_batch)\n",
    "            test_acc.append(tmp_test_acc.item())\n",
    "            \n",
    "            y_pred_tag = torch.round(y_test_pred).squeeze(1)\n",
    "            y_pred_list += list(y_pred_tag.cpu().numpy())\n",
    "            y_true_list += list(y_batch.cpu().numpy())\n",
    "        \n",
    "            \n",
    "    #correct_test_results = (np.array(y_pred_list) == np.array(y_true_list)).sum()\n",
    "    #acc = correct_test_results/len(y_true_list)\n",
    "    \n",
    "    from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "    \n",
    "    correct_labels, predicted_labels = np.array(y_true_list), np.array(y_pred_list)\n",
    "    \n",
    "    roc_auc = roc_auc_score(correct_labels, predicted_labels)\n",
    "    precision = precision_score(correct_labels, predicted_labels, average='micro')\n",
    "    recall = recall_score(correct_labels, predicted_labels, average='micro')\n",
    "    f1score = f1_score(correct_labels, predicted_labels, average='macro')\n",
    "    \n",
    "    print(f'test accuracy {np.mean(test_acc)} and ROC-AUC {roc_auc}')\n",
    "\n",
    "    #wrong_good, wrong_mal, top_k_mistaken_families = \\\n",
    "    #            get_mistaken_stats(np.array(y_true_list), np.array(np.round(y_pred_list)), Y_test_family, top_k)\n",
    "    \n",
    "    return np.mean(test_acc), roc_auc, precision, recall, f1score \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "exp_seeds = [random.randint(1, 99999) for i in range(1)]\n",
    "\n",
    "\n",
    "accs_all = []\n",
    "rocauc_all = []\n",
    "\n",
    "num_epoch = 50\n",
    "batch_size = 512\n",
    "patience = 5\n",
    "\n",
    "\n",
    "input_features = X_train.shape[1]\n",
    "\n",
    "replay_type, current_task = 'azdomain', 'azdomain'\n",
    "\n",
    "X_train = X_tr_year_transformed.toarray()\n",
    "X_test = X_te_year_transformed.toarray()\n",
    "\n",
    "Y_train = Y_tr_year\n",
    "Y_test = Y_te_year\n",
    "\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "\n",
    "for exp in exp_seeds:\n",
    "\n",
    "    start_time = time.time()\n",
    "    use_cuda = True\n",
    "    print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
    "    use_cuda = use_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    torch.manual_seed(exp)\n",
    "\n",
    "    model = Ember_MLP_Net(input_features)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.000001)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "       \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    print(f'Model has {count_parameters(model)/1000000}m parameters')    \n",
    "    criterion = nn.BCELoss()    \n",
    "\n",
    "    \n",
    "#     standardization = StandardScaler()\n",
    "#     standard_scaler = standardization.fit(X_train)\n",
    "\n",
    "#     X_train = standard_scaler.transform(X_train)\n",
    "#     X_test = standard_scaler.transform(X_test)\n",
    "    \n",
    "#     X_train, Y_train = np.array(X_train, np.float32), np.array(Y_train, np.int32)\n",
    "#     X_test, Y_test = np.array(X_test, np.float32), np.array(Y_test, np.int32)  \n",
    "\n",
    "    \n",
    "    model_save_dir = '../az_model/model/'\n",
    "    create_parent_folder(model_save_dir)\n",
    "\n",
    "    opt_save_path = '../az_model/opt/'\n",
    "    create_parent_folder(opt_save_path)\n",
    "\n",
    "    results_save_dir =  '../az_model/res/' \n",
    "    create_parent_folder(results_save_dir)\n",
    "\n",
    "    print(f'X_train {X_train.shape} Y_train {Y_train.shape}')\n",
    "    print(f'X_test {X_test.shape} Y_test {Y_test.shape}')\n",
    "    \n",
    "    \n",
    "    task_training_time, epoch_ran, training_loss, validation_loss  = training_early_stopping(\\\n",
    "                                 model, model_save_dir, opt_save_path, X_train, Y_train,\\\n",
    "                                 X_test, Y_test, patience, batch_size, device, optimizer, num_epoch,\\\n",
    "                                 criterion, replay_type, current_task, exp, earlystopping=True)\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f'Elapsed time {(end_time - start_time)/60} mins.') \n",
    "    \n",
    "    \n",
    "    \n",
    "    best_model_path = model_save_dir + os.listdir(model_save_dir)[0]\n",
    "    print(f'loading best model {best_model_path}')\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.000001)\n",
    "    best_optimizer = opt_save_path + os.listdir(opt_save_path)[0]\n",
    "    print(f'loading best optimizer {best_optimizer}')\n",
    "    optimizer.load_state_dict(torch.load(best_optimizer))\n",
    "\n",
    "\n",
    "    acc, rocauc, precision, recall, f1score = testing_aucscore(model, X_test, Y_test, batch_size, device)\n",
    "    print()\n",
    "    del model_save_dir\n",
    "    del opt_save_path\n",
    "    del results_save_dir\n",
    "    \n",
    "    accs_all.append(acc)\n",
    "    rocauc_all.append(rocauc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(accs_all), np.mean(rocauc_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with variance thresholding = 0.001\n",
    "np.mean(accs_all), np.mean(rocauc_all)\n",
    "\n",
    "# 0.001 --> 0.9644471533746527, 0.9644116198053361)\n",
    "# 0.0005 --> (0.964395448745497, 0.9644228359343698)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_X_tr, all_Y_tr, all_Y_tr_family = np.array(all_X_tr), np.array(all_Y_tr), np.array(all_Y_tr_family)\n",
    "all_X_te, all_Y_te, all_Y_te_family = np.array(all_X_te), np.array(all_Y_te), np.array(all_Y_te_family)\n",
    "\n",
    "train_raw_feat = [\" \".join(doc) for doc in all_X_tr]\n",
    "test_raw_feat = [\" \".join(doc) for doc in all_X_te]\n",
    "\n",
    "FeatureVectorizer = CountV(binary=True)\n",
    "\n",
    "train_feat_vector = FeatureVectorizer.fit_transform(train_raw_feat)\n",
    "test_feat_vector = FeatureVectorizer.transform(test_raw_feat)\n",
    "\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "selector.fit(train_feat_vector)\n",
    "\n",
    "X_train_selected = selector.transform(train_feat_vector)\n",
    "X_test_selected = selector.transform(test_feat_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_vector_all = np.array(X_train_selected.toarray(), dtype=object)\n",
    "test_vector_all = np.array(X_test_selected.toarray(), dtype=object)\n",
    "\n",
    "save_train_file = raw_path + 'All_Domain_AZ_Train.npz'\n",
    "save_test_file = raw_path + 'All_Domain_AZ_Test.npz'\n",
    "\n",
    "np.savez(save_train_file, X_train=train_vector_all, Y_train=all_Y_tr, Y_tr_family=all_Y_tr_family)\n",
    "np.savez(save_test_file, X_test=test_vector_all, Y_test=all_Y_te, Y_te_family=all_Y_te_family)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "((682598, 2115454), (75848, 2115454))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Ember_CNN(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Ember_CNN, self).__init__()\n",
    "        # Define the first convolutional layer\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Additional convolutional layers\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Dynamically calculate the size for the fully connected layer\n",
    "        self._to_linear = None\n",
    "        self.convs(torch.zeros(1, 1, num_features))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def convs(self, x):\n",
    "        # Max pooling following each convolution\n",
    "        x = F.max_pool1d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool1d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool1d(F.relu(self.conv3(x)), 2)\n",
    "        \n",
    "        # Calculate the output size for the fully connected layer\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0] * x[0].shape[1]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, -1)  # Reshape the input for Conv1d\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # Flatten the output for the fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_samples, num_features = X_train.shape\n",
    "\n",
    "# Reshape X_train to (num_samples, 1, num_features)\n",
    "X_train_reshaped = X_train.reshape(num_samples, 1, num_features)\n",
    "\n",
    "# Convert to PyTorch tensor if it's not already\n",
    "X_train_tensor = torch.tensor(X_train_reshaped, dtype=torch.float32)\n",
    "\n",
    "# Convert the NumPy array Y_train to a PyTorch tensor\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "# Create the TensorDataset using X_train_tensor and Y_train_tensor\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "\n",
    "\n",
    "\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], 1, num_features)\n",
    "X_test_tensor = torch.tensor(X_test_reshaped, dtype=torch.float32)\n",
    "\n",
    "# Convert the NumPy array Y_train to a PyTorch tensor\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "# Create the TensorDataset using X_train_tensor and Y_train_tensor\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "\n",
    "# Assuming you have training and validation datasets loaded in DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "model = Ember_CNN(num_features)  # Replace num_features with your number of features\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()  # Squeeze the output to match target labels' shape\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch2",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
