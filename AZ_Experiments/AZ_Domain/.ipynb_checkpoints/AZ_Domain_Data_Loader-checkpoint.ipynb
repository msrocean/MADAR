{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CountV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time, random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from ember_utils import *\n",
    "from ember_model import *\n",
    "from ember_pjr_utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_path = '/home/mr6564/continual_research/AZ_Data/Domain/'\n",
    "years = ['2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016']\n",
    "\n",
    "all_X_tr, all_Y_tr, all_Y_tr_family = [], [], []\n",
    "all_X_te, all_Y_te, all_Y_te_family = [], [], []\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    tr_file = raw_path + year + '_Domain_AZ_Train.npz'\n",
    "    te_file = raw_path + year + '_Domain_AZ_Test.npz'\n",
    "    \n",
    "    tr_year = np.load(tr_file, allow_pickle=True)\n",
    "    te_year = np.load(te_file, allow_pickle=True)\n",
    "    \n",
    "    X_tr_year, Y_tr_year, Y_tr_family_year = tr_year['X_train'], tr_year['Y_train'], tr_year['Y_tr_family']\n",
    "    X_te_year, Y_te_year, Y_te_family_year = te_year['X_test'], te_year['Y_test'], te_year['Y_te_family']\n",
    "    \n",
    "    \n",
    "    all_X_tr, all_Y_tr, all_Y_tr_family = np.concatenate((all_X_tr, X_tr_year)),\\\n",
    "                                            np.concatenate((all_Y_tr, Y_tr_year)),\\\n",
    "                                            np.concatenate((all_Y_tr_family, Y_tr_family_year))\n",
    "    \n",
    "    all_X_te, all_Y_te, all_Y_te_family = np.concatenate((all_X_te, X_te_year)),\\\n",
    "                                            np.concatenate((all_Y_te, Y_te_year)),\\\n",
    "                                            np.concatenate((all_Y_te_family, Y_te_family_year))\n",
    "\n",
    "#print(len(np.where(Y_tr_family_year == 'goodware')[0]), len(np.where(Y_tr_year == 0)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Data shape: (682598, 3858791)\n",
      "(75848, 3858791)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def build_vocabulary(data):\n",
    "    \"\"\"Build a vocabulary from a list of lists of strings.\"\"\"\n",
    "    vocab_set = set(word for sample in data for word in sample)\n",
    "    return sorted(list(vocab_set))  # Sort for consistency\n",
    "\n",
    "def vectorize_samples(data, vocabulary):\n",
    "    \"\"\"Vectorize data based on the given vocabulary.\"\"\"\n",
    "    vocab_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    vectorized_data = np.zeros((len(data), len(vocabulary)), dtype=int)\n",
    "\n",
    "    for i, sample in enumerate(data):\n",
    "        for word in sample:\n",
    "            if word in vocab_index:\n",
    "                vectorized_data[i, vocab_index[word]] = 1\n",
    "\n",
    "    return vectorized_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform_with_training_vocab(training_vocab_list, data_samples):\n",
    "    \"\"\"\n",
    "    Transform the data samples using the vocabulary list from the training data.\n",
    "    :param training_vocab_list: List of words from the training data\n",
    "    :param data_samples: List of data samples (each sample is a list of words)\n",
    "    :return: Vectorized data as a NumPy array\n",
    "    \"\"\"\n",
    "    # Convert the vocabulary list to a dictionary {word: index}\n",
    "    training_vocab_dict = {word: idx for idx, word in enumerate(training_vocab_list)}\n",
    "\n",
    "    # Create a zero matrix with dimensions: number of samples x size of vocabulary\n",
    "    vectorized = np.zeros((len(data_samples), len(training_vocab_list)), dtype=int)\n",
    "\n",
    "    for i, sample in enumerate(data_samples):\n",
    "        for word in sample:\n",
    "            if word in training_vocab_dict:\n",
    "                vectorized[i, training_vocab_dict[word]] = 1\n",
    "\n",
    "    return vectorized\n",
    "\n",
    "\n",
    "\n",
    "def vectorize_samples_sparse(data, vocabulary):\n",
    "    vocab_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    vectorized_data = lil_matrix((len(data), len(vocabulary)), dtype=int)\n",
    "    for i, sample in enumerate(data):\n",
    "        for word in sample:\n",
    "            if word in vocab_index:\n",
    "                vectorized_data[i, vocab_index[word]] += 1\n",
    "    return vectorized_data.tocsr()  # Convert to CSR format for efficient arithmetic and matrix vector operations\n",
    "\n",
    "\n",
    "\n",
    "def transform_with_training_vocab_sparse(training_vocab_list, data_samples):\n",
    "    training_vocab_dict = {word: idx for idx, word in enumerate(training_vocab_list)}\n",
    "    # Create a sparse matrix instead of a dense numpy array\n",
    "    vectorized = lil_matrix((len(data_samples), len(training_vocab_list)), dtype=int)\n",
    "\n",
    "    for i, sample in enumerate(data_samples):\n",
    "        for word in sample:\n",
    "            if word in training_vocab_dict:\n",
    "                vectorized[i, training_vocab_dict[word]] += 1\n",
    "    return vectorized.tocsr()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# all_X_tr, all_Y_tr, all_Y_tr_family\n",
    "# all_X_te, all_Y_te, all_Y_te_family\n",
    "\n",
    "\n",
    "data = all_X_tr\n",
    "vocabulary = build_vocabulary(data)\n",
    "vectorized_data = vectorize_samples_sparse(data, vocabulary)\n",
    "\n",
    "print(\"Vectorized Data shape:\", vectorized_data.shape)\n",
    "\n",
    "vectorized_test_data = transform_with_training_vocab_sparse(vocabulary, all_X_te)\n",
    "print(vectorized_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after variance thresholding\n",
      "(682598, 1789) (75848, 1789)\n",
      "(682598, 1789) (75848, 1789) (682598,) (75848,)\n"
     ]
    }
   ],
   "source": [
    "selector = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "selector.fit(vectorized_data)\n",
    "\n",
    "X_train_selected = selector.transform(vectorized_data)\n",
    "X_test_selected = selector.transform(vectorized_test_data)\n",
    "\n",
    "X_train, Y_train = X_train_selected, all_Y_tr\n",
    "X_test, Y_test = X_test_selected, all_Y_te\n",
    "\n",
    "\n",
    "print(f'after variance thresholding')\n",
    "print(X_train_selected.shape, X_test_selected.shape)\n",
    "\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch 2.0.1 CUDA 11.8\n",
      "Model has 2.525953m parameters\n",
      "X_train (682598, 1789) Y_train (682598,)\n",
      "X_test (75848, 1789) Y_test (75848,)\n",
      "Epoch 1 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:31<00:00, 168.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 336.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1480, Train Acc: 0.9477\n",
      "Val Loss: 0.1010, Val Acc: 0.9635\n",
      "Validation loss decreased (inf --> 0.100977).  Saving model ...\n",
      "../az_model/model/best_model_epoch_1.pt\n",
      "../az_model/opt/best_optimizer_epoch_1.pt\n",
      "Epoch 2 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:32<00:00, 162.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 327.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1111, Train Acc: 0.9620\n",
      "Val Loss: 0.0925, Val Acc: 0.9675\n",
      "Validation loss decreased (0.100977 --> 0.092510).  Saving model ...\n",
      "../az_model/model/best_model_epoch_2.pt\n",
      "../az_model/opt/best_optimizer_epoch_2.pt\n",
      "Epoch 3 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:31<00:00, 167.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 327.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0971, Train Acc: 0.9676\n",
      "Val Loss: 0.0905, Val Acc: 0.9684\n",
      "Validation loss decreased (0.092510 --> 0.090538).  Saving model ...\n",
      "../az_model/model/best_model_epoch_3.pt\n",
      "../az_model/opt/best_optimizer_epoch_3.pt\n",
      "Epoch 4 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:36<00:00, 147.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 327.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0866, Train Acc: 0.9715\n",
      "Val Loss: 0.0867, Val Acc: 0.9701\n",
      "Validation loss decreased (0.090538 --> 0.086743).  Saving model ...\n",
      "../az_model/model/best_model_epoch_4.pt\n",
      "../az_model/opt/best_optimizer_epoch_4.pt\n",
      "Epoch 5 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:33<00:00, 160.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 331.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0800, Train Acc: 0.9737\n",
      "Val Loss: 0.0928, Val Acc: 0.9688\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 6 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:35<00:00, 149.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 333.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0752, Train Acc: 0.9752\n",
      "Val Loss: 0.0823, Val Acc: 0.9719\n",
      "Validation loss decreased (0.086743 --> 0.082280).  Saving model ...\n",
      "../az_model/model/best_model_epoch_6.pt\n",
      "../az_model/opt/best_optimizer_epoch_6.pt\n",
      "Epoch 7 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:31<00:00, 168.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 331.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0720, Train Acc: 0.9765\n",
      "Val Loss: 0.0832, Val Acc: 0.9730\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 8 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:29<00:00, 180.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:02<00:00, 262.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0680, Train Acc: 0.9780\n",
      "Val Loss: 0.0787, Val Acc: 0.9754\n",
      "Validation loss decreased (0.082280 --> 0.078710).  Saving model ...\n",
      "../az_model/model/best_model_epoch_8.pt\n",
      "../az_model/opt/best_optimizer_epoch_8.pt\n",
      "Epoch 9 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:35<00:00, 152.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 305.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0650, Train Acc: 0.9789\n",
      "Val Loss: 0.0817, Val Acc: 0.9747\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 10 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:29<00:00, 180.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:02<00:00, 284.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0636, Train Acc: 0.9795\n",
      "Val Loss: 0.0828, Val Acc: 0.9748\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 11 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:38<00:00, 139.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 310.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0614, Train Acc: 0.9802\n",
      "Val Loss: 0.0843, Val Acc: 0.9748\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 12 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:33<00:00, 161.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 312.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0596, Train Acc: 0.9808\n",
      "Val Loss: 0.0865, Val Acc: 0.9748\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 13 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:35<00:00, 151.11it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 311.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0577, Train Acc: 0.9816\n",
      "Val Loss: 0.0817, Val Acc: 0.9752\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 14 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:38<00:00, 138.56it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 316.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0561, Train Acc: 0.9821\n",
      "Val Loss: 0.0807, Val Acc: 0.9755\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 15 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:35<00:00, 148.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:02<00:00, 292.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0555, Train Acc: 0.9823\n",
      "Val Loss: 0.0831, Val Acc: 0.9739\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 16 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:39<00:00, 134.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:02<00:00, 279.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0547, Train Acc: 0.9827\n",
      "Val Loss: 0.0802, Val Acc: 0.9748\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 17 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:38<00:00, 138.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:01<00:00, 299.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0537, Train Acc: 0.9830\n",
      "Val Loss: 0.0797, Val Acc: 0.9756\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 18 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5332/5332 [00:35<00:00, 151.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:02<00:00, 282.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0526, Train Acc: 0.9833\n",
      "Val Loss: 0.0823, Val Acc: 0.9754\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "Training time: 10.942 minutes\n",
      "Elapsed time 10.954123024145762 mins.\n",
      "loading best model ../az_model/model/best_model_epoch_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 593/593 [00:02<00:00, 251.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.963860783267383 and ROC-AUC 0.9639617336301808\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rocauc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 148\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m results_save_dir\n\u001b[1;32m    147\u001b[0m accs_all\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[0;32m--> 148\u001b[0m rocauc_all\u001b[38;5;241m.\u001b[39mappend(\u001b[43mrocauc\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rocauc' is not defined"
     ]
    }
   ],
   "source": [
    "class Ember_MLP_Net(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(Ember_MLP_Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_features, 1024)\n",
    "        self.fc1_bn = nn.BatchNorm1d(1024)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc1_drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc2_bn = nn.BatchNorm1d(512)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.fc2_drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc3_bn = nn.BatchNorm1d(256)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.fc3_drop = nn.Dropout(p=0.5)        \n",
    "        \n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc4_bn = nn.BatchNorm1d(128)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.fc4_drop = nn.Dropout(p=0.5)  \n",
    "        \n",
    "        self.fc_last = nn.Linear(128, 1) \n",
    "        self.out = nn.Sigmoid()\n",
    "        \n",
    "        #self.activate = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.act1(x) \n",
    "        x = self.fc1_drop(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2_bn(x)\n",
    "        x = self.act2(x) \n",
    "        x = self.fc2_drop(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.fc3_bn(x)\n",
    "        x = self.act3(x) \n",
    "        x = self.fc3_drop(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.fc4_bn(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.fc4_drop(x)\n",
    "        \n",
    "        x = self.fc_last(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "exp_seeds = [random.randint(1, 99999) for i in range(2)]\n",
    "\n",
    "\n",
    "accs_all = []\n",
    "rocauc_all = []\n",
    "\n",
    "num_epoch = 50\n",
    "batch_size = 128\n",
    "patience = 10\n",
    "\n",
    "\n",
    "input_features = X_train.shape[1]\n",
    "\n",
    "replay_type, current_task = 'azdomain', 'azdomain'\n",
    "\n",
    "for exp in exp_seeds:\n",
    "\n",
    "    start_time = time.time()\n",
    "    use_cuda = True\n",
    "    print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
    "    use_cuda = use_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    torch.manual_seed(exp)\n",
    "\n",
    "    model = Ember_MLP_Net(input_features)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.000001)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "       \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    print(f'Model has {count_parameters(model)/1000000}m parameters')    \n",
    "    criterion = nn.BCELoss()    \n",
    "\n",
    "    \n",
    "#     standardization = StandardScaler()\n",
    "#     standard_scaler = standardization.fit(X_train)\n",
    "\n",
    "#     X_train = standard_scaler.transform(X_train)\n",
    "#     X_test = standard_scaler.transform(X_test)\n",
    "    \n",
    "#     X_train, Y_train = np.array(X_train, np.float32), np.array(Y_train, np.int32)\n",
    "#     X_test, Y_test = np.array(X_test, np.float32), np.array(Y_test, np.int32)  \n",
    "\n",
    "    \n",
    "    model_save_dir = '../az_model/model/'\n",
    "    create_parent_folder(model_save_dir)\n",
    "\n",
    "    opt_save_path = '../az_model/opt/'\n",
    "    create_parent_folder(opt_save_path)\n",
    "\n",
    "    results_save_dir =  '../az_model/res/' \n",
    "    create_parent_folder(results_save_dir)\n",
    "\n",
    "    print(f'X_train {X_train.shape} Y_train {Y_train.shape}')\n",
    "    print(f'X_test {X_test.shape} Y_test {Y_test.shape}')\n",
    "    \n",
    "    \n",
    "    task_training_time, epoch_ran, training_loss, validation_loss  = training_early_stopping(\\\n",
    "                                 model, model_save_dir, opt_save_path, X_train, Y_train,\\\n",
    "                                 X_test, Y_test, patience, batch_size, device, optimizer, num_epoch,\\\n",
    "                                 criterion, replay_type, current_task, exp, earlystopping=True)\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f'Elapsed time {(end_time - start_time)/60} mins.') \n",
    "    \n",
    "    \n",
    "    \n",
    "    best_model_path = model_save_dir + os.listdir(model_save_dir)[0]\n",
    "    print(f'loading best model {best_model_path}')\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.000001)\n",
    "    best_optimizer = opt_save_path + os.listdir(opt_save_path)[0]\n",
    "    print(f'loading best optimizer {best_optimizer}')\n",
    "    optimizer.load_state_dict(torch.load(best_optimizer))\n",
    "\n",
    "\n",
    "    acc, rocauc, precision, recall, f1score = testing_aucscore(model, X_test, Y_test, batch_size, device)\n",
    "    print()\n",
    "    del model_save_dir\n",
    "    del opt_save_path\n",
    "    del results_save_dir\n",
    "    \n",
    "    accs_all.append(acc)\n",
    "    rocauc_all.append(rocauc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_X_tr, all_Y_tr, all_Y_tr_family = np.array(all_X_tr), np.array(all_Y_tr), np.array(all_Y_tr_family)\n",
    "all_X_te, all_Y_te, all_Y_te_family = np.array(all_X_te), np.array(all_Y_te), np.array(all_Y_te_family)\n",
    "\n",
    "train_raw_feat = [\" \".join(doc) for doc in all_X_tr]\n",
    "test_raw_feat = [\" \".join(doc) for doc in all_X_te]\n",
    "\n",
    "FeatureVectorizer = CountV(binary=True)\n",
    "\n",
    "train_feat_vector = FeatureVectorizer.fit_transform(train_raw_feat)\n",
    "test_feat_vector = FeatureVectorizer.transform(test_raw_feat)\n",
    "\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "selector.fit(train_feat_vector)\n",
    "\n",
    "X_train_selected = selector.transform(train_feat_vector)\n",
    "X_test_selected = selector.transform(test_feat_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_vector_all = np.array(X_train_selected.toarray(), dtype=object)\n",
    "test_vector_all = np.array(X_test_selected.toarray(), dtype=object)\n",
    "\n",
    "save_train_file = raw_path + 'All_Domain_AZ_Train.npz'\n",
    "save_test_file = raw_path + 'All_Domain_AZ_Test.npz'\n",
    "\n",
    "np.savez(save_train_file, X_train=train_vector_all, Y_train=all_Y_tr, Y_tr_family=all_Y_tr_family)\n",
    "np.savez(save_test_file, X_test=test_vector_all, Y_test=all_Y_te, Y_te_family=all_Y_te_family)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "((682598, 2115454), (75848, 2115454))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Ember_CNN(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Ember_CNN, self).__init__()\n",
    "        # Define the first convolutional layer\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Additional convolutional layers\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Dynamically calculate the size for the fully connected layer\n",
    "        self._to_linear = None\n",
    "        self.convs(torch.zeros(1, 1, num_features))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def convs(self, x):\n",
    "        # Max pooling following each convolution\n",
    "        x = F.max_pool1d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool1d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool1d(F.relu(self.conv3(x)), 2)\n",
    "        \n",
    "        # Calculate the output size for the fully connected layer\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0] * x[0].shape[1]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, -1)  # Reshape the input for Conv1d\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # Flatten the output for the fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_samples, num_features = X_train.shape\n",
    "\n",
    "# Reshape X_train to (num_samples, 1, num_features)\n",
    "X_train_reshaped = X_train.reshape(num_samples, 1, num_features)\n",
    "\n",
    "# Convert to PyTorch tensor if it's not already\n",
    "X_train_tensor = torch.tensor(X_train_reshaped, dtype=torch.float32)\n",
    "\n",
    "# Convert the NumPy array Y_train to a PyTorch tensor\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "# Create the TensorDataset using X_train_tensor and Y_train_tensor\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "\n",
    "\n",
    "\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], 1, num_features)\n",
    "X_test_tensor = torch.tensor(X_test_reshaped, dtype=torch.float32)\n",
    "\n",
    "# Convert the NumPy array Y_train to a PyTorch tensor\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "# Create the TensorDataset using X_train_tensor and Y_train_tensor\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "\n",
    "# Assuming you have training and validation datasets loaded in DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "model = Ember_CNN(num_features)  # Replace num_features with your number of features\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()  # Squeeze the output to match target labels' shape\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch2",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
